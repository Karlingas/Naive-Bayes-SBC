{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "V01Ub14OGcRJ"
   },
   "source": [
    "<br><br><br>\n",
    "<h2><font size=6>Práctica 2</font></h2>\n",
    "\n",
    "\n",
    "\n",
    "<h1><font size=7>Naive Bayes</font></h1>\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: right\">\n",
    "<font size=4>Diego García Díaz (Diego.Garcia30@alu.uclm.es)</font><br>\n",
    "<font size=4>Alberto Pérez Álvarez (Alberto.Perez25@alu.uclm.es)</font><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"#B30033\" size=5>Estudiantes: </font>** \n",
    "\n",
    "* Diego García Díaz\n",
    "* Alberto Pérez Álvarez "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "B-Yy-FwTG33Y"
   },
   "source": [
    "---\n",
    "\n",
    "<a id=\"introduccion\"></a>\n",
    "# 1. Introducción\n",
    "\n",
    "<br>\n",
    "\n",
    "El objetivo de esta práctica es estudiar el algoritmo [`Naive Bayes`](https://scikit-learn.org/stable/modules/naive_bayes.html) para clasificación. Para ello, se deberá:\n",
    "1. Llevar a cabo un estudio de los clasificadores ya implementados de la librería `scikit-learn`. En él, se estudiará tanto la versión de Naive Bayes para variables discretas, [`CategoricalNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html), como la versión Gaussiana para variables continuas, [`GaussianNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html). En el caso de `CategoricalNB`, además, se estudiará el efecto de la discretización de variables sobre los resultados.\n",
    "2. Implementación del algoritmo Naive Bayes mixto (que pueda tratar tanto variables discretas como continuas) en Python y realización de una comparación con los utilizados de `scikit-learn`.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5vR6t4WXGpmb"
   },
   "source": [
    "---\n",
    "\n",
    "<a id=\"indice\"></a>\n",
    "<h2>Índice</h2>\n",
    "\n",
    "* [1. Introducción](#introduccion)\n",
    "* [2. Modelos Gráficos Probabilísticos](#pgm)\n",
    "* [3. Naive Bayes en scikit-learn](#naive-scikit)\n",
    "* [4. Implementación de Naive Bayes categórico](#implementacion)\n",
    "* [5. Evaluación experimental](#evaluacion)\n",
    "* [6. Trabajo opcional](#opcional)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"pgm\"></a>\n",
    "# 2. Modelos Gráficos Probabilísticos\n",
    "\n",
    "<br>\n",
    "\n",
    "## 2.1 Clasificación probabilística\n",
    "\n",
    "Como ya habéis visto en clase, podemos realizar una predicción de las probabilidades a posteriori de los posibles valores $\\{c_1,\\dots,c_k\\}$ de una variable $y$ mediante el cálculo de $P(y\\ |\\ x_1,\\dots,x_n)$, donde $x_1,\\dots,x_n$ son las $n$ variables independientes usadas para estimar $y$. Una vez contamos con las probabilidades, si queremos realizar una clasificación simplemente habrá que devolver el valor $\\{c_1,\\dots,c_k\\}$ con mayor probabilidad, es decir, $\\hat{y} = arg\\ max_{y \\in \\{c_1,\\dots,c_k\\}} P(y\\ |\\ x_1,\\dots,x_n)$.\n",
    "\n",
    "\n",
    "En la práctica, es imposible realizar dicho cálculo en la mayoría de ocasiones ya que aún contando con variables binarias se necesitaría crear una **Distribución de Probabilidad Conjunta (DCP)** en la que se almacene la probabilidad de cada combinación, es decir, $2^n$ valores.\n",
    "\n",
    "Podemos intentar reducir el tamaño de la DCP. Usando el **Teorema de Bayes**, tenemos que: $$P(y\\ |\\ x_1,\\dots,x_n) = \\frac{P(y)P(x_1,\\dots,x_n\\ |\\ y)}{P(x_1,\\dots,x_n)} = \\frac{P(y,x_1,\\dots,x_n)}{P(x_1,\\dots,x_n)}$$\n",
    "\n",
    "Ya que el denomiador va a ser constante independiententemente de $c$, lo podemos eliminar, quedándonos lo siguiente: $$P(y\\ |\\ x_1,\\dots,x_n) \\propto P(y)P(x_1,\\dots,x_n\\ |\\ y) = P(y,x_1,\\dots,x_n)$$\n",
    "\n",
    "Esta conversión es fundamental para las **Redes Bayesianas**, ya que gracias a ella podemos aplicar la **regla de la cadena** y la **Condición de Markov** para poder almacenar eficientemente la Distribución de Probabilidad Conjunta. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "kiYVXSxiYNNw"
   },
   "source": [
    "## 2.2 Redes Bayesianas\n",
    "\n",
    "<img src=\"Imagenes/BN.png\" width=\"600\">\n",
    "\n",
    "Pese a la reducción considerable en cuanto a complejidad que conseguimos con los métodos comentados anteriormente, el aprendizaje de una Red Bayesiana sigue siendo un problema NP-Hard. Este aprendizaje se divide en dos fases:\n",
    "\n",
    "- **Aprendizaje estructural**: Obtener la **estructura gráfica** de la Red Bayesiana:\n",
    "\n",
    "> En la práctica, suele ser la parte más compleja. Se suelen utilizar algoritmos voraces en los que se limita lo máximo posible el espacio de búsqueda de los enlaces a añadir. Por ejemplo, en una Red Bayesiana con 1000 variables, para obtener el resultado óptimo habría que probar todas las combinaciones posibles de los aproximadamente 1.000.000 enlaces que se pueden añadir.\n",
    "- **Aprendizaje paramétrico**: Obtener las **tablas de probabilidades** asociadas a cada variable:\n",
    "\n",
    "> Su complejidad se reduce mucho gracias a la factorización, ya que para cada variable solo hay que añadir a dichas tablas sus variables padres. Es decir, para la Red Bayesiana de ejemplo de la imagen superior, para la variable $X_6$ será necesario almacenar $P(X_6,X_2,X_3$); para $X_7$, $P(X_7,X_1)$; y para $X_1$, $P(X_1)$. En este ejemplo dichas tablas son pequeñas, pero sin embargo, si en nuestra red está densamente conectada (es decir, cada variable tiene un gran número de padres), la complejidad tanto temporal como espacial de esta fase del aprendizaje se puede disparar, ya que para variables binarias la tabla asociada a cada variable tendrá $2^n$ valores (si las variables en vez de binarias tuviesen por ejemplo 3 valores, sería $3^n$, y así sucesivamente).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.3 Naive Bayes\n",
    "\n",
    "<img src=\"Imagenes/Naive.png\" width=\"800\">\n",
    "\n",
    "**Naive Bayes** es un algoritmo de clasificación probabilística que permite solucionar los problemas en cuanto a la complejidad de las Redes Bayesianas, ya que por un lado al contar con una **estructura fija** no es necesario realizar **aprendizaje estructural**, y por otro cada variable únicamente va a tener un padre (la variable clase), por lo que su **aprendizaje paramétrico** también es muy simple.\n",
    "\n",
    "Así, se factoriza la Distribución de Probabilidad Conjunta de tal modo que finalmente contamos con: $$P(y\\ |\\ x_1,\\dots,x_n) \\propto P(y)\\prod_{i=1}^{n}P(x_i\\ |\\ y)$$ \n",
    "\n",
    "Por lo que podremos realizar predicciones con: $$\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n}P(x_i\\ |\\ y)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.4 Estimación de las probabilidades\n",
    "\n",
    "Por tanto, necesitamos estimar las distribuciones de probabilidad condicional $P(x_i\\ |\\ y)$ para cada variable $x_i$​ dado el valor de la clase $y$, además de la distribución de la clase $P(y)$. Esto se hace de manera diferente para variables discretas y continuas:\n",
    "\n",
    "* **Variables discretas:** Las podemos estimar con las frecuencias relativas del conjunto de entrenamiento. Así, podemos calcular $P(x_i\\ |\\ y)$ contando las frecuencias de cada valor de $x_i$ para cada valor de la clase $y$ en nuestros datos, y luego normalizándolos para obtener probabilidades.\n",
    "\n",
    "* **Variables continuas:** Para estas variables generalmente se asume una distribución de probabilidad específica, como la distribución normal (Gaussiana). Entonces, para cada variable continua $x_i$, estimamos los parámetros de esta distribución (como pueden ser la media y desviación estándar) utilizando los datos de entrenamiento correspondientes a cada valor de la clase $y$. Una vez que se han estimado estos parámetros, podemos usarlos para calcular la probabilidad condicional $P(x_i\\ |\\ y)$ para cada valor de $y$ con una nueva observación $x_i$​."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"naive-scikit\"></a>\n",
    "# 3 [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html) en `scikit-learn`\n",
    "\n",
    "\n",
    "`scikit-learn` cuenta con cinco implementaciones distintas de Naive Bayes: [`CategoricalNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html), [`GaussianNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html), [`BernoulliNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html), [`MultinomialNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) y [`ComplementNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.ComplementNB.html). En nuestro caso, nos vamos a centrar en `CategoricalNB` y `GaussianNB`, las dos versiones más utilizadas, y que están relacionadas con el código que se deberá implementar (el resto se utilizan principalmente para predicción en textos).\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## 3.1 `CategoricalNB`\n",
    "\n",
    "`CategoricalNB` es la implementación de Naive Bayes para variables discretas, explicada en la Sección 2.3 y que tendréis que implementar en la Sección 4. Siguiendo la estructura típica de `scikit-learn`, cuenta con un método `fit(X,y)` para entrenar el modelo y un `predict(X)` para realizar las predicciones. Además, caben destacar otros métodos como `partial_fit(X,y)`, para añadir datos a un modelo ya entrenado al ser un modelo incremental; `predict_proba(X)`, para predecir la probabilidades de cada clase para cada instancia de $X$ en lugar de únicamente obtener la clase predicha; y `score(X,y)`, que devuelve el porcentaje de acierto del modelo dadas unas instancias $X$ y las clases reales $y$.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a probar algunos de estos métodos con la base de datos `iris` (la podemos cargar directamente desde `scikit-learn`). Primero importamos todo lo necesario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "id": "WJlZrshbbZvc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "import utilsP2 as utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero vamos a cargar la base de datos a un dataframe de Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el conjunto de datos desde scikit-learn\n",
    "iris = load_iris()\n",
    "\n",
    "# Crear un dataframe de Pandas con los datos y las etiquetas\n",
    "df_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df_iris['target'] = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_iris.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y vamos a dividir los datos en entrenamiento ($X_{train}$ e $y_{train}$) y test ($X_{test}$ e $y_{test}$). Esto no fue necesario en la Práctica 1 ya que el conjunto `adult` nos proporcionaba un `.csv` distinto para entrenamiento y test.\n",
    "\n",
    "Mostramos las 5 primeras instancias de los conjuntos de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5. , 2. , 3.5, 1. ],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.7, 2.5, 5.8, 1.8]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = utils.train_test(df_iris)\n",
    "\n",
    "X_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como estamos probando `CategoricalNB` y nuestras variables son continuas, vamos a discretizarlas con `KBinsDiscretizer` antes de crear el modelo. En este caso, hemos elegido un discretizado uniforme en 3 bins, por lo que si aplicamos el discretizado todos los valores de las variables serán 0, 1 o 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 1.],\n",
       "       [1., 1., 2., 2.],\n",
       "       [2., 1., 2., 2.],\n",
       "       [1., 0., 2., 1.],\n",
       "       [2., 0., 2., 2.]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Discretizar X \n",
    "est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
    "\n",
    "X_train_disc = est.fit_transform(X_train)\n",
    "X_test_disc = est.transform(X_test)\n",
    "X_train_disc[0:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora sí, entrenamos nuestro `CategoricalNB` con `fit(X)`, y calculamos el score del algoritmo con `score(X,y)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentaje de acierto en entrenamiento:  0.9428571428571428\n",
      "Porcentaje de acierto en prueba:  0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Crear un modelo NB\n",
    "categoricalNB = CategoricalNB()\n",
    " \n",
    "# Entrenar el modelo\n",
    "categoricalNB.fit(X_train_disc, y_train)\n",
    "\n",
    "# Calcular el porcentaje de acierto\n",
    "scoreTrain = categoricalNB.score(X_train_disc, y_train)\n",
    "scoreTest = categoricalNB.score(X_test_disc, y_test)\n",
    "print(\"Porcentaje de acierto en entrenamiento: \", scoreTrain)\n",
    "print(\"Porcentaje de acierto en prueba: \", scoreTest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar `predict(X)` para ver las predicciones realizadas para cada instancia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions Train:\n",
      " [1,2,2,1,2,1,2,1,1,1,1,1,2,1,2,1,0,2,1,1,1,1,2,0,0,2,1,0,0,1,0,2,1,0,1,2,1,0,2,2,2,2,0,0,2,2,0,2,0,2,2,0,0,\n",
      " 2,0,0,0,1,2,2,0,0,0,1,1,0,0,1,0,2,1,2,1,0,2,0,2,0,0,2,0,2,1,1,1,2,2,1,2,0,1,2,2,0,1,1,2,1,0,0,0,2,1,2,0] \n",
      "\n",
      "Predictions Test:\n",
      " [2,1,0,2,0,2,0,1,1,1,1,1,1,1,1,0,1,1,0,0,1,1,0,0,1,0,0,1,1,0,2,1,0,2,2,1,0,1,1,1,2,0,2,0,0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predecir las etiquetas de los datos de entrenamiento. \n",
    "predictionsTrain = categoricalNB.predict(X_train_disc)\n",
    "print(\"Predictions Train:\\n\", np.array2string(predictionsTrain, separator=',', max_line_width=107), \"\\n\")\n",
    "\n",
    "# Predecir las etiquetas de los datos de prueba. Mostrar 25 números por linea\n",
    "predictionsTest = categoricalNB.predict(X_test_disc)\n",
    "print(\"Predictions Test:\\n\", np.array2string(predictionsTest, separator=',', max_line_width=107), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y los podríamos comparar con los valores reales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train:\n",
      " [1,2,2,2,2,1,2,1,1,2,2,2,2,1,2,1,0,2,1,1,1,1,2,0,0,2,1,0,0,1,0,2,1,0,1,2,1,0,2,2,2,2,0,0,2,2,0,2,0,2,2,0,0,\n",
      " 2,0,0,0,1,2,2,0,0,0,1,1,0,0,1,0,2,1,2,1,0,2,0,2,0,0,2,0,2,1,1,1,2,2,1,1,0,1,2,2,0,1,1,1,1,0,0,0,2,1,2,0] \n",
      "\n",
      "y_test:\n",
      " [2,1,0,2,0,2,0,1,1,1,2,1,1,1,1,0,1,1,0,0,2,1,0,0,2,0,0,1,1,0,2,1,0,2,2,1,0,1,1,1,2,0,2,0,0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"y_train:\\n\", np.array2string(y_train, separator=',', max_line_width=107), \"\\n\")\n",
    "print(\"y_test:\\n\", np.array2string(y_test, separator=',', max_line_width=107), \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y también podemos ver las probalidades predichas para cada instancia con `predict_proba(X)` (mostramos solo las 5 primeras intancias):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.83088646e-04, 9.95159326e-01, 4.05758490e-03],\n",
       "       [2.27560421e-04, 6.41968548e-03, 9.93352754e-01],\n",
       "       [5.71964381e-05, 1.24120253e-03, 9.98701601e-01],\n",
       "       [1.23476706e-04, 5.26427262e-01, 4.73449262e-01],\n",
       "       [7.03293093e-06, 2.23206016e-03, 9.97760907e-01]])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categoricalNB.predict_proba(X_train_disc)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA:** En este caso es muy importante la discretización. Si probáis con 2 bins, veréis como el resultado es mucho peor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 `GaussianNB`\n",
    "\n",
    "`GaussianNB` es la implementación de Naive Bayes para variables continuas estimadas como gausianas. Básicamente, cuenta con los mismos métodos principales que `CategoricalNB`.\n",
    "\n",
    "Vamos a realizar el mismo proceso de prueba con la base `iris` que antes, en este caso sin discretizar las variables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos nuestro `GaussianNB` con `fit(X)`, y calculamos el score del algoritmo con `score(X,y)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentaje de acierto en entrenamiento:  0.9428571428571428\n",
      "Porcentaje de acierto en prueba:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Crear un modelo NB\n",
    "gaussianNB = GaussianNB()\n",
    " \n",
    "# Entrenar el modelo\n",
    "gaussianNB.fit(X_train, y_train)\n",
    "\n",
    "# Calcular el porcentaje de acierto\n",
    "scoreTrain = gaussianNB.score(X_train, y_train)\n",
    "scoreTest = gaussianNB.score(X_test, y_test)\n",
    "print(\"Porcentaje de acierto en entrenamiento: \", scoreTrain)\n",
    "print(\"Porcentaje de acierto en prueba: \", scoreTest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar `predict(X)` para ver las predicciones realizadas para cada instancia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions Train:\n",
      " [1,2,2,1,2,1,2,1,1,2,1,1,2,1,2,1,0,2,1,1,1,1,2,0,0,2,1,0,0,2,0,2,1,0,1,2,1,0,2,2,2,2,0,0,2,2,0,2,0,2,2,0,0,\n",
      " 2,0,0,0,1,2,2,0,0,0,1,1,0,0,1,0,2,1,2,1,0,2,0,2,0,0,2,0,2,1,1,1,2,2,1,2,0,1,2,2,0,1,1,2,1,0,0,0,2,1,2,0] \n",
      "\n",
      "Predictions Test:\n",
      " [2,1,0,2,0,2,0,1,1,1,2,1,1,1,1,0,1,1,0,0,2,1,0,0,2,0,0,1,1,0,2,1,0,2,2,1,0,1,1,1,2,0,2,0,0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predecir las etiquetas de los datos de entrenamiento. \n",
    "predictionsTrain = gaussianNB.predict(X_train)\n",
    "print(\"Predictions Train:\\n\", np.array2string(predictionsTrain, separator=',', max_line_width=107), \"\\n\")\n",
    "\n",
    "# Predecir las etiquetas de los datos de prueba. Mostrar 25 números por linea\n",
    "predictionsTest = gaussianNB.predict(X_test)\n",
    "print(\"Predictions Test:\\n\", np.array2string(predictionsTest, separator=',', max_line_width=107), \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y también podemos ver las probalidades predichas para cada instancia con `predict_proba(X)` (mostramos solo las 5 primeras intancias):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.02882530e-059, 9.99999927e-001, 7.25939137e-008],\n",
       "       [1.07057258e-241, 1.44292097e-003, 9.98557079e-001],\n",
       "       [0.00000000e+000, 3.62956541e-010, 1.00000000e+000],\n",
       "       [4.82432659e-178, 9.51750980e-001, 4.82490196e-002],\n",
       "       [2.04990326e-270, 3.70855956e-004, 9.99629144e-001]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussianNB.predict_proba(X_train)[0:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, podemos ver cómo el rendimiento de `GaussianNB` es mejor que el de `CategoricalNB`. Esto dependerá en gran medida de la base de datos y del discretizado que se realice. Además, la base `iris` es muy pequeña, por lo que estos scores tampoco son representativos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"implementacion\"></a>\n",
    "# 4. `TODO:` Implementación de Naive Bayes mixto\n",
    "\n",
    "<br>\n",
    "\n",
    "En esta sección, implementaremos un clasificador **Naive Bayes mixto**, capaz de manejar tanto variables **categóricas** como **numéricas**. A diferencia de los modelos de `scikit-learn`, que separan `CategoricalNB` y `GaussianNB`, nuestra implementación combinará ambos enfoques en una única clase `NB`, que hereda de `BaseEstimator`. Esto nos permitirá trabajar con datasets que contengan ambos tipos de variables de manera flexible.\n",
    "\n",
    "### Características:\n",
    "\n",
    "1. **Combinación de variables categóricas y numéricas**:\n",
    "   - Las variables categóricas se modelan utilizando probabilidades condicionales discretas.\n",
    "   - Las variables numéricas se modelan utilizando distribuciones gaussianas (media y varianza).\n",
    "\n",
    "2. **Parámetro `categorical_features`**:\n",
    "   - Este parámetro permite especificar qué variables se tratarán como categóricas.\n",
    "   - Puede tomar los siguientes valores:\n",
    "     - Una lista de índices (por ejemplo, `[0, 2, 5]`).\n",
    "     - `'all'` (valor por defecto): Todas las variables se tratan como categóricas.\n",
    "     - `'none'`: Todas las variables se tratan como numéricas.\n",
    "\n",
    "3. **Suavizado de Laplace**:\n",
    "   - Para evitar probabilidades condicionales iguales a cero en las variables categóricas, se aplica el **suavizado de Laplace**.\n",
    "   - Esto es especialmente importante cuando hay categorías en los datos de entrenamiento que no aparecen en algunas clases.\n",
    "   - Si no lo aplicamos, no obtendríamos los mismos resultados que el `CategoricalNB` de `scikit-learn`.\n",
    "\n",
    "4. **Funciones que se deben implementar**: (cada una tiene sus respectivos `TODO`)\n",
    "   - **`fit(self, X, y)`**: Entrena el modelo con los datos de entrenamiento `X` y las etiquetas `y`.\n",
    "   - **`predict(self, X)`**: Predice las etiquetas para los datos de prueba `X`.\n",
    "   - **`predict_proba(self, X)`**: Devuelve las probabilidades de cada clase para los datos de prueba `X`.\n",
    "   - **`score(self, X, y)`**: Calcula la precisión del modelo en los datos de prueba `X` y las etiquetas `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sympy.abc import y\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Implementación propia de Naive Bayes que hereda de BaseEstimator siguiendo la estructura de scikit-learn\n",
    "# Las guias de scikit-learn para la implementación de algoritmos se encuentran en https://scikit-learn.org/stable/developers/develop.html\n",
    "class NB(BaseEstimator):\n",
    "    # Constructor\n",
    "    def __init__(self, categorical_features='all'):\n",
    "        self.categorical_features = categorical_features\n",
    "        self.epsilon = 1e-9 \n",
    "        return\n",
    "\n",
    "    # To string\n",
    "    def __str__(self):\n",
    "        result = \"NB(): Naive Bayes with Categorical and Gaussian Features\\n\\n\"\n",
    "        result += \"P(Y):\\n\" + str(np.round(self.class_prob_, 4)) + \"\\n\\n\"\n",
    "        \n",
    "        for i in range(len(self.tables_)):\n",
    "            if self.is_categorical_[i]:  # Característica categórica\n",
    "                result += f\"P(X{i}|Y):\\n\"\n",
    "                for cls, probs in self.tables_[i].items():\n",
    "                    result += f\"{cls}:\\t{np.round(probs, 4)}\\n\"\n",
    "                result += \"\\n\"\n",
    "            else:  # Característica gaussiana\n",
    "                result += f\"P(X{i}|Y):\\n\"\n",
    "                for cls, (mean, var) in self.tables_[i].items():\n",
    "                    result += f\"{cls}:\\tμ = {mean:.4f}, σ² = {var:.4f}\\n\"\n",
    "                result += \"\\n\"\n",
    "        return result\n",
    "\n",
    "\n",
    "    # Función fit para entrenar el modelo a partir de un conjunto de datos X y sus etiquetas y\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        # ... (rest of feature detection and class info is the same)\n",
    "        if isinstance(self.categorical_features, str):\n",
    "            if self.categorical_features == 'all':\n",
    "                self.categorical_features_ = list(range(n_features))\n",
    "            elif self.categorical_features == 'none':\n",
    "                self.categorical_features_ = []\n",
    "            else:\n",
    "                raise ValueError(\"categorical_features must be 'all', 'none', or a list of indices.\")\n",
    "        elif isinstance(self.categorical_features, (list, np.ndarray)):\n",
    "             self.categorical_features_ = sorted(list(set(f for f in self.categorical_features if 0 <= f < n_features)))\n",
    "        else:\n",
    "             raise ValueError(\"categorical_features must be 'all', 'none', or a list of indices.\")\n",
    "\n",
    "        self.is_categorical_ = np.isin(range(n_features), self.categorical_features_)\n",
    "\n",
    "        self.classes_, class_counts = np.unique(y, return_counts=True)\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "        self.class_prob_ = class_counts / n_samples\n",
    "\n",
    "        self.categories_ = [None] * n_features\n",
    "        self.category_to_index_ = [None] * n_features\n",
    "        self.tables_ = [None] * n_features\n",
    "\n",
    "        # --- Internal Data Cleaning/Conversion (Revised Approach) ---\n",
    "        # Process each column and store the result in a list.\n",
    "        # We will assemble the final X_processed array from this list later.\n",
    "        processed_cols = []\n",
    "        for i in range(n_features):\n",
    "             col_data = X[:, i] # Get the column data (from original X)\n",
    "\n",
    "             if self.is_categorical_[i]:\n",
    "                 # Categorical columns: Keep as is (object, string, or int are fine)\n",
    "                 processed_cols.append(col_data)\n",
    "                 print(f\"--- Feature {i} (Categorical): Appending column with Dtype = {col_data.dtype} ---\")\n",
    "             else: # Continuous features\n",
    "                 # Continuous columns: ALWAYS convert to float64\n",
    "                 print(f\"--- Processing Feature {i} (continuous) ---\")\n",
    "                 print(f\"Before element-wise conversion (from original X): Dtype = {col_data.dtype}, Sample = {col_data[:5]}\")\n",
    "\n",
    "                 # Perform element-wise conversion to float, handling errors\n",
    "                 converted_col_list = []\n",
    "                 for value in col_data:\n",
    "                     try:\n",
    "                         converted_col_list.append(float(value))\n",
    "                     except (ValueError, TypeError):\n",
    "                         # If conversion fails, use NaN\n",
    "                         converted_col_list.append(np.nan)\n",
    "\n",
    "                 # Create a NEW NumPy array from the list, explicitly setting dtype to float64\n",
    "                 processed_col_array = np.array(converted_col_list, dtype=np.float64)\n",
    "                 processed_cols.append(processed_col_array) # Add this new array to our list\n",
    "\n",
    "                 print(f\"After element-wise conversion (processed column array): Dtype = {processed_col_array.dtype}, Sample = {processed_col_array[:5]}\")\n",
    "\n",
    "        # Assemble the processed columns into the final X_processed array\n",
    "        # np.column_stack correctly handles combining arrays of different dtypes\n",
    "        # provided they are compatible (e.g., mixing object and float64 is fine)\n",
    "        X_processed = np.column_stack(processed_cols)\n",
    "        print(f\"\\n--- Assembled X_processed ---\")\n",
    "        print(f\"Final X_processed Shape: {X_processed.shape}\")\n",
    "        print(f\"Final X_processed Dtype: {X_processed.dtype}\") # This will be 'object' if any col is 'object', but the underlying data for float cols is float64\n",
    "        # Check dtypes of individual columns in the assembled array (more informative)\n",
    "        for i in range(n_features):\n",
    "             print(f\"  Column {i} Final Dtype: {X_processed[:, i].dtype}\")\n",
    "        print(\"-----------------------------\")\n",
    "\n",
    "        # --- End Internal Data Cleaning ---\n",
    "\n",
    "\n",
    "        # Now, the rest of the fit method uses X_processed\n",
    "        # The key is that when we slice X_processed[:, i] for a continuous feature,\n",
    "        # the underlying data for that specific column should now be float64,\n",
    "        # even if the overall X_processed array's main dtype is 'object'.\n",
    "        for i in range(n_features):\n",
    "            if self.is_categorical_[i]:\n",
    "                 # ... (rest of categorical fit logic using X_processed[:, i]) ...\n",
    "                 unique_categories = np.unique(X_processed[:, i])\n",
    "                 self.categories_[i] = unique_categories\n",
    "                 self.category_to_index_[i] = {cat: idx for idx, cat in enumerate(unique_categories)}\n",
    "\n",
    "                 feature_table = {}\n",
    "                 num_categories = len(unique_categories)\n",
    "\n",
    "                 for k, cls in enumerate(self.classes_):\n",
    "                     X_cls = X_processed[y == cls, i]\n",
    "                     n_cls = len(X_cls)\n",
    "\n",
    "                     category_counts = defaultdict(int)\n",
    "                     for cat_value in X_cls:\n",
    "                          if cat_value is not None and cat_value is not np.nan: # Handle potential NaNs/None\n",
    "                             category_counts[cat_value] += 1\n",
    "\n",
    "                     probs = []\n",
    "                     for cat_value in unique_categories:\n",
    "                          count = category_counts[cat_value]\n",
    "                          smoothed_prob = (count + 1) / (n_cls + num_categories)\n",
    "                          probs.append(smoothed_prob)\n",
    "\n",
    "                     feature_table[cls] = probs\n",
    "\n",
    "                 self.tables_[i] = feature_table\n",
    "\n",
    "            else: # Continuous features\n",
    "                feature_table = {}\n",
    "                for k, cls in enumerate(self.classes_):\n",
    "                    X_cls = X_processed[y == cls, i] # Slice should now be float64\n",
    "\n",
    "                    # Use nanmean and nanvar which ignore NaNs\n",
    "                    mean = np.nanmean(X_cls)\n",
    "                    var = np.nanvar(X_cls, ddof=1)\n",
    "\n",
    "                    # Add epsilon after calculating base variance. Handle case where variance is NaN or 0.\n",
    "                    if np.isnan(var) or var <= self.epsilon: # Use <= epsilon to handle zero variance\n",
    "                         var = self.epsilon\n",
    "                    else:\n",
    "                         var += self.epsilon # Add epsilon for stability if variance is positive\n",
    "\n",
    "                    feature_table[cls] = [mean, var]\n",
    "\n",
    "                self.tables_[i] = feature_table\n",
    "\n",
    "        return self # ... (rest of fit method) ...\n",
    "\n",
    "\n",
    "    # predict_proba_matrix method - apply the same revised cleaning\n",
    "    def predict_proba_matrix(self, X):\n",
    "        num_instances, n_features = X.shape\n",
    "\n",
    "        # --- Internal Data Cleaning/Conversion (Revised Approach) ---\n",
    "        processed_cols = []\n",
    "        for i in range(n_features):\n",
    "             col_data = X[:, i]\n",
    "\n",
    "             if self.is_categorical_[i]:\n",
    "                 processed_cols.append(col_data)\n",
    "                 print(f\"--- Feature {i} (Categorical): Appending column with Dtype = {col_data.dtype} ---\")\n",
    "             else: # Continuous features\n",
    "                 print(f\"--- Processing Prediction Feature {i} (continuous) ---\")\n",
    "                 print(f\"Before element-wise conversion (from original X): Dtype = {col_data.dtype}, Sample = {col_data[:5]}\")\n",
    "\n",
    "                 converted_col_list = []\n",
    "                 for value in col_data:\n",
    "                     try:\n",
    "                         converted_col_list.append(float(value))\n",
    "                     except (ValueError, TypeError):\n",
    "                         converted_col_list.append(np.nan)\n",
    "\n",
    "                 processed_col_array = np.array(converted_col_list, dtype=np.float64)\n",
    "                 processed_cols.append(processed_col_array)\n",
    "\n",
    "                 print(f\"After element-wise conversion (processed column array): Dtype = {processed_col_array.dtype}, Sample = {processed_col_array[:5]}\")\n",
    "\n",
    "        X_processed = np.column_stack(processed_cols)\n",
    "        print(f\"\\n--- Assembled X_processed (Prediction) ---\")\n",
    "        print(f\"Final X_processed Shape: {X_processed.shape}\")\n",
    "        print(f\"Final X_processed Dtype: {X_processed.dtype}\")\n",
    "        for i in range(n_features):\n",
    "             print(f\"  Column {i} Final Dtype: {X_processed[:, i].dtype}\")\n",
    "        print(\"---------------------------------------\")\n",
    "\n",
    "        # --- End Internal Data Cleaning ---\n",
    "\n",
    "        log_probs = np.zeros((num_instances, self.n_classes_))\n",
    "        log_probs += np.log(self.class_prob_)\n",
    "\n",
    "        for j in range(n_features):\n",
    "            for k, cls in enumerate(self.classes_):\n",
    "                # The issue with dtype('O') should now be resolved for continuous features\n",
    "                # because X_processed was assembled with correctly typed columns.\n",
    "\n",
    "                if self.is_categorical_[j]:\n",
    "                    # ... (rest of categorical likelihoods using X_processed[:, j]) ...\n",
    "                    probs_list = self.tables_[j][cls]\n",
    "                    try:\n",
    "                         instance_category_indices = np.array([\n",
    "                               self.category_to_index_[j].get(val, -1)\n",
    "                               if val is not None and val is not np.nan else -1 # Explicitly map None/NaN to -1\n",
    "                               for val in X_processed[:, j]\n",
    "                         ])\n",
    "                    except TypeError:\n",
    "                         instance_category_indices = np.array([\n",
    "                             self.category_to_index_[j].get(val, -1)\n",
    "                             if val is not None and val is not np.nan else -1\n",
    "                             for val in X_processed[:, j].tolist()\n",
    "                         ])\n",
    "\n",
    "\n",
    "                    log_likelihoods = np.full(num_instances, np.log(self.epsilon))\n",
    "                    seen_mask = instance_category_indices != -1\n",
    "                    seen_indices_in_X = np.where(seen_mask)[0]\n",
    "                    seen_category_indices = instance_category_indices[seen_mask]\n",
    "\n",
    "                    if len(seen_category_indices) > 0:\n",
    "                        # np.take should work fine on float64 probs_list with integer indices\n",
    "                        # Check log(prob) is not -inf or NaN if prob was 0 or NaN\n",
    "                        prob_values = np.take(probs_list, seen_category_indices)\n",
    "                        # Handle cases where smoothed prob was 0 (shouldn't happen with Laplace, but defensive)\n",
    "                        prob_values[prob_values <= 0] = self.epsilon # Replace non-positive with tiny epsilon\n",
    "                        log_likelihoods[seen_indices_in_X] = np.log(prob_values)\n",
    "\n",
    "\n",
    "                    log_probs[:, k] += log_likelihoods # This addition should work if log_likelihoods is float64\n",
    "\n",
    "                else: # Continuous features\n",
    "                    mean, var = self.tables_[j][cls]\n",
    "                    # var already includes epsilon from fit\n",
    "\n",
    "                    col_data_processed = X_processed[:, j] # This slice should now be float64\n",
    "\n",
    "                    # Calculate log probability density (vectorized)\n",
    "                    # This should now work because col_data_processed is float64 (possibly with NaNs)\n",
    "                    # Add robustness for var potentially being very small or NaN from fit (already handled)\n",
    "                    # Add robustness for col_data_processed causing issues (NaNs are handled by arithmetic)\n",
    "                    # Avoid log(0) if var was <= 0 before adding epsilon (handled in fit)\n",
    "\n",
    "                    # Ensure denominator in exponent is not zero or negative\n",
    "                    safe_var = var # Already handled var<=epsilon in fit\n",
    "\n",
    "                    # Calculate (x - mu)^2 / (2 * sigma^2) safely\n",
    "                    diff = col_data_processed - mean\n",
    "                    # np.log(safe_var) is safe if var > 0 after epsilon\n",
    "                    log_likelihoods = -0.5 * np.log(2 * np.pi * safe_var) - 0.5 * (diff**2 / safe_var)\n",
    "\n",
    "                    # Handle cases where log_likelihoods might become NaN or Inf if input was NaN or calculation resulted in it\n",
    "                    # NumPy arithmetic on NaN inputs usually results in NaN outputs, which is fine.\n",
    "                    # If diff**2 / safe_var results in Inf or a very large number, log_likelihoods can be -Inf.\n",
    "                    # These extreme values are typically handled by log-sum-exp trick.\n",
    "\n",
    "                    log_probs[:, k] += log_likelihoods # This addition should work if log_likelihoods is float64\n",
    "\n",
    "\n",
    "        # Apply log-sum-exp trick and normalize\n",
    "        # np.max and np.sum handle NaNs appropriately. max of [-inf, -inf] is -inf. sum involving NaN is NaN.\n",
    "        max_log_probs = np.max(log_probs, axis=1, keepdims=True)\n",
    "        # Replace -inf max_log_probs with a large negative number or 0 before subtracting to avoid +inf\n",
    "        max_log_probs[np.isinf(max_log_probs) & (max_log_probs < 0)] = 0 # Or some other large negative number\n",
    "\n",
    "        # Handle rows where all log_probs were NaN or -inf (max_log_probs might be NaN or 0 from above)\n",
    "        # If max_log_probs is NaN, subtracting it results in NaN.\n",
    "        # If all log_probs were -inf, max_log_probs might be 0 or -inf. If 0, exp is 1/inf. If -inf, exp is 0.\n",
    "        # Let's ensure we don't get NaN from subtraction if the original row was all non-NaN finite/inf numbers\n",
    "        # The standard log-sum-exp is robust if max_log_probs is finite.\n",
    "\n",
    "        exp_probs = np.exp(log_probs - max_log_probs) # This can produce NaNs if log_probs or max_log_probs is NaN\n",
    "        sum_exp_probs = np.sum(exp_probs, axis=1, keepdims=True)\n",
    "\n",
    "        # Avoid division by zero or division by NaN\n",
    "        # Replace 0 sum with 1 to yield 0 probability after division for that instance\n",
    "        # Replace NaN sum with NaN itself (or 1 if you want NaN instances to result in NaN probabilities)\n",
    "        sum_exp_probs[sum_exp_probs == 0] = 1\n",
    "        # If sum_exp_probs is NaN, the result will be NaN / NaN -> NaN, which is fine.\n",
    "\n",
    "\n",
    "        prob_matrix = exp_probs / sum_exp_probs\n",
    "\n",
    "        # Ensure the output is float64, just in case\n",
    "        return prob_matrix.astype(np.float64)\n",
    "\n",
    "    # predict_proba and predict methods remain the same, calling predict_proba_matrix\n",
    "    def predict_proba(self, X):\n",
    "         prob_matrix = self.predict_proba_matrix(X)\n",
    "         p = np.max(prob_matrix, axis=1)\n",
    "         # Handle argmax of rows that might be all NaN or 0 after normalization (shouldn't happen with laplace + epsilon)\n",
    "         # If a row is all NaN, argmax raises ValueError. If all 0, argmax is 0.\n",
    "         # A robust argmax might handle NaN rows explicitly. For now, assume normalization prevents all NaNs if any original log_prob was finite.\n",
    "         c = self.classes_[np.argmax(prob_matrix, axis=1)]\n",
    "         return (p, c)\n",
    "\n",
    "    \n",
    "\n",
    "    # Función predict para predecir las etiquetas de un conjunto de datos X\n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X)[1]\n",
    "\n",
    "\n",
    "    # Función score para calcular el porcentaje de acierto del modelo a partir de un conjunto de datos X y sus etiquetas y\n",
    "    def score(self, X, y):\n",
    "        correctos = 0\n",
    "        resultado = self.predict(X)\n",
    "        for i in range(len(X)):\n",
    "            if  resultado[i] == y[i]:\n",
    "                correctos += 1\n",
    "        return correctos/len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.seterr(divide='ignore') # Ignorar los avisos de divisiones por 0 que muestra CategoricalNB con alpha=0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"evaluacion\"></a>\n",
    "# 5. Evaluación experimental\n",
    "\n",
    "<br>\n",
    "\n",
    "##  5.1 Pruebas en la BBDD `iris`\n",
    "\n",
    "En primer lugar, vamos a comprobar que efectivamente nuestro nuevo clasificador `NB` funciona correctamente en la base de datos `iris`. Esta base es muy útil para comparar las tablas de probabilidad, ya que tendrán un tamaño pequeño."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  5.1.1 Variables categóricas\n",
    "\n",
    "Primero vamos a probar con las variables categóricas (discretizando):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Feature 0 (Categorical): Appending column with Dtype = float64 ---\n",
      "--- Feature 1 (Categorical): Appending column with Dtype = float64 ---\n",
      "--- Feature 2 (Categorical): Appending column with Dtype = float64 ---\n",
      "--- Feature 3 (Categorical): Appending column with Dtype = float64 ---\n",
      "\n",
      "--- Assembled X_processed ---\n",
      "Final X_processed Shape: (105, 4)\n",
      "Final X_processed Dtype: float64\n",
      "  Column 0 Final Dtype: float64\n",
      "  Column 1 Final Dtype: float64\n",
      "  Column 2 Final Dtype: float64\n",
      "  Column 3 Final Dtype: float64\n",
      "-----------------------------\n",
      "Fit es correcto\n",
      "--- Feature 0 (Categorical): Appending column with Dtype = float64 ---\n",
      "--- Feature 1 (Categorical): Appending column with Dtype = float64 ---\n",
      "--- Feature 2 (Categorical): Appending column with Dtype = float64 ---\n",
      "--- Feature 3 (Categorical): Appending column with Dtype = float64 ---\n",
      "\n",
      "--- Assembled X_processed (Prediction) ---\n",
      "Final X_processed Shape: (105, 4)\n",
      "Final X_processed Dtype: float64\n",
      "  Column 0 Final Dtype: float64\n",
      "  Column 1 Final Dtype: float64\n",
      "  Column 2 Final Dtype: float64\n",
      "  Column 3 Final Dtype: float64\n",
      "---------------------------------------\n",
      "\n",
      "Longitud: 105\n",
      "Longitud esperada: 105\n",
      "\n",
      "Resultado: [1 2 2 1 2 1 2 1 1 1 1 1 2 1 2 1 0 2 1 1 1 1 2 0 0 2 1 0 0 1 0 2 1 0 1 2 1\n",
      " 0 2 2 2 2 0 0 2 2 0 2 0 2 2 0 0 2 0 0 0 1 2 2 0 0 0 1 1 0 0 1 0 2 1 2 1 0\n",
      " 2 0 2 0 0 2 0 2 1 1 1 2 2 1 2 0 1 2 2 0 1 1 2 1 0 0 0 2 1 2 0]\n",
      "Esperado:  [1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 0, 2, 1, 1, 1, 1, 2, 0, 0, 2, 1, 0, 0, 1, 0, 2, 1, 0, 1, 2, 1, 0, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 0, 2, 2, 0, 0, 2, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 1, 0, 0, 1, 0, 2, 1, 2, 1, 0, 2, 0, 2, 0, 0, 2, 0, 2, 1, 1, 1, 2, 2, 1, 2, 0, 1, 2, 2, 0, 1, 1, 2, 1, 0, 0, 0, 2, 1, 2, 0]\n",
      "Los predict son correctos: True\n",
      "--- Feature 0 (Categorical): Appending column with Dtype = float64 ---\n",
      "--- Feature 1 (Categorical): Appending column with Dtype = float64 ---\n",
      "--- Feature 2 (Categorical): Appending column with Dtype = float64 ---\n",
      "--- Feature 3 (Categorical): Appending column with Dtype = float64 ---\n",
      "\n",
      "--- Assembled X_processed (Prediction) ---\n",
      "Final X_processed Shape: (105, 4)\n",
      "Final X_processed Dtype: float64\n",
      "  Column 0 Final Dtype: float64\n",
      "  Column 1 Final Dtype: float64\n",
      "  Column 2 Final Dtype: float64\n",
      "  Column 3 Final Dtype: float64\n",
      "---------------------------------------\n",
      "\n",
      "Score train: 0.9428571428571428\n",
      "--- Feature 0 (Categorical): Appending column with Dtype = float64 ---\n",
      "--- Feature 1 (Categorical): Appending column with Dtype = float64 ---\n",
      "--- Feature 2 (Categorical): Appending column with Dtype = float64 ---\n",
      "--- Feature 3 (Categorical): Appending column with Dtype = float64 ---\n",
      "\n",
      "--- Assembled X_processed (Prediction) ---\n",
      "Final X_processed Shape: (45, 4)\n",
      "Final X_processed Dtype: float64\n",
      "  Column 0 Final Dtype: float64\n",
      "  Column 1 Final Dtype: float64\n",
      "  Column 2 Final Dtype: float64\n",
      "  Column 3 Final Dtype: float64\n",
      "---------------------------------------\n",
      "Score test:  0.9333333333333333 \n",
      "\n",
      "--- Feature 0 (Categorical): Appending column with Dtype = float64 ---\n",
      "--- Feature 1 (Categorical): Appending column with Dtype = float64 ---\n",
      "--- Feature 2 (Categorical): Appending column with Dtype = float64 ---\n",
      "--- Feature 3 (Categorical): Appending column with Dtype = float64 ---\n",
      "\n",
      "--- Assembled X_processed (Prediction) ---\n",
      "Final X_processed Shape: (105, 4)\n",
      "Final X_processed Dtype: float64\n",
      "  Column 0 Final Dtype: float64\n",
      "  Column 1 Final Dtype: float64\n",
      "  Column 2 Final Dtype: float64\n",
      "  Column 3 Final Dtype: float64\n",
      "---------------------------------------\n",
      "\n",
      "Predictions train:\n",
      " [1,2,2,1,2,1,2,1,1,1,1,1,2,1,2,1,0,2,1,1,1,1,2,0,0,2,1,0,0,1,0,2,1,0,1,2,1,0,2,2,2,2,0,0,2,2,0,2,0,2,2,0,0,\n",
      " 2,0,0,0,1,2,2,0,0,0,1,1,0,0,1,0,2,1,2,1,0,2,0,2,0,0,2,0,2,1,1,1,2,2,1,2,0,1,2,2,0,1,1,2,1,0,0,0,2,1,2,0] \n",
      "\n",
      "--- Feature 0 (Categorical): Appending column with Dtype = float64 ---\n",
      "--- Feature 1 (Categorical): Appending column with Dtype = float64 ---\n",
      "--- Feature 2 (Categorical): Appending column with Dtype = float64 ---\n",
      "--- Feature 3 (Categorical): Appending column with Dtype = float64 ---\n",
      "\n",
      "--- Assembled X_processed (Prediction) ---\n",
      "Final X_processed Shape: (45, 4)\n",
      "Final X_processed Dtype: float64\n",
      "  Column 0 Final Dtype: float64\n",
      "  Column 1 Final Dtype: float64\n",
      "  Column 2 Final Dtype: float64\n",
      "  Column 3 Final Dtype: float64\n",
      "---------------------------------------\n",
      "Predictions test:\n",
      " [2,1,0,2,0,2,0,1,1,1,1,1,1,1,1,0,1,1,0,0,1,1,0,0,1,0,0,1,1,0,2,1,0,2,2,1,0,1,1,1,2,0,2,0,0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "expected = [1,2,2,1,2,1,2,1,1,1,1,1,2,1,2,1,0,2,1,1,1,1,2,0,0,2,1,0,0,1,0,2,1,0,1,2,1,0,2,2,2,2,0,0,2,2,0,2,0,2,2,0,0,2,0,0,0,1,2,2,0,0,0,1,1,0,0,1,0,2,1,2,1,0,2,0,2,0,0,2,0,2,1,1,1,2,2,1,2,0,1,2,2,0,1,1,2,1,0,0,0,2,1,2,0]\n",
    "\n",
    "# Crear un modelo NB categórico\n",
    "nb = NB(categorical_features='all')\n",
    "\n",
    "# Entrenar el modelo\n",
    "nb.fit(X_train_disc, y_train)\n",
    "#utils.see_probability_tables(nb)\n",
    "print(\"Fit es correcto\")\n",
    "p = nb.predict(X_train_disc)\n",
    "\n",
    "print(\"\\nLongitud:\",len(p))\n",
    "print(\"Longitud esperada:\",len(expected))\n",
    "\n",
    "print(\"\\nResultado:\",p)\n",
    "print(\"Esperado: \",expected)\n",
    "print(\"Los predict son correctos:\",np.any(expected == p))\n",
    "\n",
    "#print(y_train)\n",
    "# Calcular el porcentaje de acierto\n",
    "print(\"\\nScore train:\", nb.score(X_train_disc, y_train))\n",
    "print(\"Score test: \", nb.score(X_test_disc, y_test), \"\\n\")\n",
    "\n",
    "# Predecir las etiquetas de los datos de entrenamiento. \n",
    "print(\"\\nPredictions train:\\n\", np.array2string(nb.predict(X_train_disc), separator=',', max_line_width=107), \"\\n\")\n",
    "print(\"Predictions test:\\n\", np.array2string(nb.predict(X_test_disc), separator=',', max_line_width=107), \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el método `see_probability_tables(nb)` de `utils` podemos ver las tablas de probabilidades que ha creado el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Y):\n",
      " [0.32380952 0.3047619  0.37142857] \n",
      "\n",
      "P(X0|Y):\n",
      "0:\t[0.8649 0.1081 0.027 ]\n",
      "1:\t[0.1143 0.7429 0.1429]\n",
      "2:\t[0.0476 0.4762 0.4762]\n",
      "\n",
      "P(X1|Y):\n",
      "0:\t[0.0541 0.7027 0.2432]\n",
      "1:\t[0.5143 0.4571 0.0286]\n",
      "2:\t[0.3571 0.5714 0.0714]\n",
      "\n",
      "P(X2|Y):\n",
      "0:\t[0.9459 0.027  0.027 ]\n",
      "1:\t[0.0571 0.8857 0.0571]\n",
      "2:\t[0.0238 0.0952 0.881 ]\n",
      "\n",
      "P(X3|Y):\n",
      "0:\t[0.9459 0.027  0.027 ]\n",
      "1:\t[0.0286 0.8857 0.0857]\n",
      "2:\t[0.0238 0.0952 0.881 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utils.see_probability_tables(nb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora podemos realizar lo mismo con `CategoricalNB` para comprobar que los resultados son los mismos: \n",
    "\n",
    "NOTA: Si no implementamos el suavizado de Laplace, deberemos llamar a `CategoricalNB(alpha=0, force_alpha=True)` para obtener el mismo resultado. Esto puede que no os funcione dependiendo de vuestra versión de `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train: 0.9428571428571428\n",
      "Score test:  0.9333333333333333 \n",
      "\n",
      "Predictions train:\n",
      " [1,2,2,1,2,1,2,1,1,1,1,1,2,1,2,1,0,2,1,1,1,1,2,0,0,2,1,0,0,1,0,2,1,0,1,2,1,0,2,2,2,2,0,0,2,2,0,2,0,2,2,0,0,\n",
      " 2,0,0,0,1,2,2,0,0,0,1,1,0,0,1,0,2,1,2,1,0,2,0,2,0,0,2,0,2,1,1,1,2,2,1,2,0,1,2,2,0,1,1,2,1,0,0,0,2,1,2,0] \n",
      "\n",
      "Predictions test:\n",
      " [2,1,0,2,0,2,0,1,1,1,1,1,1,1,1,0,1,1,0,0,1,1,0,0,1,0,0,1,1,0,2,1,0,2,2,1,0,1,1,1,2,0,2,0,0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comparar con el modelo CategoricalNB de scikit\n",
    "nb = CategoricalNB()\n",
    "nb.fit(X_train_disc, y_train)\n",
    "\n",
    "# Calcular el porcentaje de acierto\n",
    "print(\"Score train:\", nb.score(X_train_disc, y_train))\n",
    "print(\"Score test: \", nb.score(X_test_disc, y_test), \"\\n\")\n",
    "\n",
    "# Predecir las etiquetas de los datos de entrenamiento. \n",
    "print(\"Predictions train:\\n\", np.array2string(nb.predict(X_train_disc), separator=',', max_line_width=107), \"\\n\")\n",
    "print(\"Predictions test:\\n\", np.array2string(nb.predict(X_test_disc), separator=',', max_line_width=107), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Y):\n",
      " [0.32380952 0.3047619  0.37142857] \n",
      "\n",
      "P(X0|Y):\n",
      "0:\t[0.8649 0.1081 0.027 ]\n",
      "1:\t[0.1143 0.7429 0.1429]\n",
      "2:\t[0.0476 0.4762 0.4762]\n",
      "\n",
      "P(X1|Y):\n",
      "0:\t[0.0541 0.7027 0.2432]\n",
      "1:\t[0.5143 0.4571 0.0286]\n",
      "2:\t[0.3571 0.5714 0.0714]\n",
      "\n",
      "P(X2|Y):\n",
      "0:\t[0.9459 0.027  0.027 ]\n",
      "1:\t[0.0571 0.8857 0.0571]\n",
      "2:\t[0.0238 0.0952 0.881 ]\n",
      "\n",
      "P(X3|Y):\n",
      "0:\t[0.9459 0.027  0.027 ]\n",
      "1:\t[0.0286 0.8857 0.0857]\n",
      "2:\t[0.0238 0.0952 0.881 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utils.see_probability_tables(nb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos comprobar, el resultado es el mismo. Si queremos comprobar más bins, podemos utilizar la función `compare_two_models` de utils para comparar los accuracy en test de `NB` y `CategoricalNB` con distintos discretizados, en este caso igual anchura desde 2 hasta 10 bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Variables continuas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing Feature 0 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = float64, Sample = [5.  6.5 6.7 6.  6.7]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [5.  6.5 6.7 6.  6.7]\n",
      "--- Processing Feature 1 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = float64, Sample = [2.  3.  3.3 2.2 2.5]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [2.  3.  3.3 2.2 2.5]\n",
      "--- Processing Feature 2 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = float64, Sample = [3.5 5.5 5.7 5.  5.8]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [3.5 5.5 5.7 5.  5.8]\n",
      "--- Processing Feature 3 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = float64, Sample = [1.  1.8 2.5 1.5 1.8]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [1.  1.8 2.5 1.5 1.8]\n",
      "\n",
      "--- Assembled X_processed ---\n",
      "Final X_processed Shape: (105, 4)\n",
      "Final X_processed Dtype: float64\n",
      "  Column 0 Final Dtype: float64\n",
      "  Column 1 Final Dtype: float64\n",
      "  Column 2 Final Dtype: float64\n",
      "  Column 3 Final Dtype: float64\n",
      "-----------------------------\n",
      "--- Processing Prediction Feature 0 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = float64, Sample = [5.  6.5 6.7 6.  6.7]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [5.  6.5 6.7 6.  6.7]\n",
      "--- Processing Prediction Feature 1 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = float64, Sample = [2.  3.  3.3 2.2 2.5]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [2.  3.  3.3 2.2 2.5]\n",
      "--- Processing Prediction Feature 2 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = float64, Sample = [3.5 5.5 5.7 5.  5.8]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [3.5 5.5 5.7 5.  5.8]\n",
      "--- Processing Prediction Feature 3 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = float64, Sample = [1.  1.8 2.5 1.5 1.8]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [1.  1.8 2.5 1.5 1.8]\n",
      "\n",
      "--- Assembled X_processed (Prediction) ---\n",
      "Final X_processed Shape: (105, 4)\n",
      "Final X_processed Dtype: float64\n",
      "  Column 0 Final Dtype: float64\n",
      "  Column 1 Final Dtype: float64\n",
      "  Column 2 Final Dtype: float64\n",
      "  Column 3 Final Dtype: float64\n",
      "---------------------------------------\n",
      "\n",
      "Los predict en train son correctos:  True \n",
      "\n",
      "--- Processing Prediction Feature 0 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = float64, Sample = [5.8 6.  5.5 7.3 5. ]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [5.8 6.  5.5 7.3 5. ]\n",
      "--- Processing Prediction Feature 1 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = float64, Sample = [2.8 2.2 4.2 2.9 3.4]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [2.8 2.2 4.2 2.9 3.4]\n",
      "--- Processing Prediction Feature 2 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = float64, Sample = [5.1 4.  1.4 6.3 1.5]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [5.1 4.  1.4 6.3 1.5]\n",
      "--- Processing Prediction Feature 3 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = float64, Sample = [2.4 1.  0.2 1.8 0.2]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [2.4 1.  0.2 1.8 0.2]\n",
      "\n",
      "--- Assembled X_processed (Prediction) ---\n",
      "Final X_processed Shape: (45, 4)\n",
      "Final X_processed Dtype: float64\n",
      "  Column 0 Final Dtype: float64\n",
      "  Column 1 Final Dtype: float64\n",
      "  Column 2 Final Dtype: float64\n",
      "  Column 3 Final Dtype: float64\n",
      "---------------------------------------\n",
      "\n",
      "Los predict en test son correctos:  True \n",
      "\n",
      "--- Processing Prediction Feature 0 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = float64, Sample = [5.  6.5 6.7 6.  6.7]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [5.  6.5 6.7 6.  6.7]\n",
      "--- Processing Prediction Feature 1 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = float64, Sample = [2.  3.  3.3 2.2 2.5]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [2.  3.  3.3 2.2 2.5]\n",
      "--- Processing Prediction Feature 2 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = float64, Sample = [3.5 5.5 5.7 5.  5.8]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [3.5 5.5 5.7 5.  5.8]\n",
      "--- Processing Prediction Feature 3 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = float64, Sample = [1.  1.8 2.5 1.5 1.8]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [1.  1.8 2.5 1.5 1.8]\n",
      "\n",
      "--- Assembled X_processed (Prediction) ---\n",
      "Final X_processed Shape: (105, 4)\n",
      "Final X_processed Dtype: float64\n",
      "  Column 0 Final Dtype: float64\n",
      "  Column 1 Final Dtype: float64\n",
      "  Column 2 Final Dtype: float64\n",
      "  Column 3 Final Dtype: float64\n",
      "---------------------------------------\n",
      "Score train: 0.9428571428571428\n",
      "--- Processing Prediction Feature 0 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = float64, Sample = [5.8 6.  5.5 7.3 5. ]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [5.8 6.  5.5 7.3 5. ]\n",
      "--- Processing Prediction Feature 1 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = float64, Sample = [2.8 2.2 4.2 2.9 3.4]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [2.8 2.2 4.2 2.9 3.4]\n",
      "--- Processing Prediction Feature 2 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = float64, Sample = [5.1 4.  1.4 6.3 1.5]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [5.1 4.  1.4 6.3 1.5]\n",
      "--- Processing Prediction Feature 3 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = float64, Sample = [2.4 1.  0.2 1.8 0.2]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [2.4 1.  0.2 1.8 0.2]\n",
      "\n",
      "--- Assembled X_processed (Prediction) ---\n",
      "Final X_processed Shape: (45, 4)\n",
      "Final X_processed Dtype: float64\n",
      "  Column 0 Final Dtype: float64\n",
      "  Column 1 Final Dtype: float64\n",
      "  Column 2 Final Dtype: float64\n",
      "  Column 3 Final Dtype: float64\n",
      "---------------------------------------\n",
      "Score test:  1.0 \n",
      "\n",
      "--- Processing Prediction Feature 0 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = float64, Sample = [5.  6.5 6.7 6.  6.7]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [5.  6.5 6.7 6.  6.7]\n",
      "--- Processing Prediction Feature 1 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = float64, Sample = [2.  3.  3.3 2.2 2.5]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [2.  3.  3.3 2.2 2.5]\n",
      "--- Processing Prediction Feature 2 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = float64, Sample = [3.5 5.5 5.7 5.  5.8]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [3.5 5.5 5.7 5.  5.8]\n",
      "--- Processing Prediction Feature 3 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = float64, Sample = [1.  1.8 2.5 1.5 1.8]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [1.  1.8 2.5 1.5 1.8]\n",
      "\n",
      "--- Assembled X_processed (Prediction) ---\n",
      "Final X_processed Shape: (105, 4)\n",
      "Final X_processed Dtype: float64\n",
      "  Column 0 Final Dtype: float64\n",
      "  Column 1 Final Dtype: float64\n",
      "  Column 2 Final Dtype: float64\n",
      "  Column 3 Final Dtype: float64\n",
      "---------------------------------------\n",
      "Predictions train:\n",
      " [1,2,2,1,2,1,2,1,1,2,1,1,2,1,2,1,0,2,1,1,1,1,2,0,0,2,1,0,0,2,0,2,1,0,1,2,1,0,2,2,2,2,0,0,2,2,0,2,0,2,2,0,0,\n",
      " 2,0,0,0,1,2,2,0,0,0,1,1,0,0,1,0,2,1,2,1,0,2,0,2,0,0,2,0,2,1,1,1,2,2,1,2,0,1,2,2,0,1,1,2,1,0,0,0,2,1,2,0] \n",
      "\n",
      "--- Processing Prediction Feature 0 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = float64, Sample = [5.8 6.  5.5 7.3 5. ]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [5.8 6.  5.5 7.3 5. ]\n",
      "--- Processing Prediction Feature 1 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = float64, Sample = [2.8 2.2 4.2 2.9 3.4]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [2.8 2.2 4.2 2.9 3.4]\n",
      "--- Processing Prediction Feature 2 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = float64, Sample = [5.1 4.  1.4 6.3 1.5]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [5.1 4.  1.4 6.3 1.5]\n",
      "--- Processing Prediction Feature 3 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = float64, Sample = [2.4 1.  0.2 1.8 0.2]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [2.4 1.  0.2 1.8 0.2]\n",
      "\n",
      "--- Assembled X_processed (Prediction) ---\n",
      "Final X_processed Shape: (45, 4)\n",
      "Final X_processed Dtype: float64\n",
      "  Column 0 Final Dtype: float64\n",
      "  Column 1 Final Dtype: float64\n",
      "  Column 2 Final Dtype: float64\n",
      "  Column 3 Final Dtype: float64\n",
      "---------------------------------------\n",
      "Predictions test:\n",
      " [2,1,0,2,0,2,0,1,1,1,2,1,1,1,1,0,1,1,0,0,2,1,0,0,2,0,0,1,1,0,2,1,0,2,2,1,0,1,1,1,2,0,2,0,0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear un modelo NB categórico\n",
    "nb = NB(categorical_features='none')\n",
    "\n",
    "# Entrenar el modelo\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nLos predict en train son correctos: \",(nb.score(X_train, y_train) == 0.9428571428571428),\"\\n\")\n",
    "print(\"\\nLos predict en test son correctos: \",(nb.score(X_test, y_test) == 1.0),\"\\n\")\n",
    "\n",
    "\n",
    "# Calcular el porcentaje de acierto\n",
    "print(\"Score train:\", nb.score(X_train, y_train))\n",
    "print(\"Score test: \", nb.score(X_test, y_test), \"\\n\")\n",
    "\n",
    "# Predecir las etiquetas de los datos de entrenamiento. \n",
    "print(\"Predictions train:\\n\", np.array2string(nb.predict(X_train), separator=',', max_line_width=107), \"\\n\")\n",
    "print(\"Predictions test:\\n\", np.array2string(nb.predict(X_test), separator=',', max_line_width=107), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Y):\n",
      " [0.32380952 0.3047619  0.37142857] \n",
      "\n",
      "P(X0|Y):\n",
      "0:\tμ = 4.9941, σ² = 0.1297\n",
      "1:\tμ = 5.9219, σ² = 0.2902\n",
      "2:\tμ = 6.6538, σ² = 0.4373\n",
      "\n",
      "P(X1|Y):\n",
      "0:\tμ = 3.3824, σ² = 0.1579\n",
      "1:\tμ = 2.7563, σ² = 0.1193\n",
      "2:\tμ = 2.9872, σ² = 0.1233\n",
      "\n",
      "P(X2|Y):\n",
      "0:\tμ = 1.4529, σ² = 0.0207\n",
      "1:\tμ = 4.1969, σ² = 0.2416\n",
      "2:\tμ = 5.5974, σ² = 0.3224\n",
      "\n",
      "P(X3|Y):\n",
      "0:\tμ = 0.2324, σ² = 0.0089\n",
      "1:\tμ = 1.3062, σ² = 0.0432\n",
      "2:\tμ = 2.0308, σ² = 0.0690\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utils.see_probability_tables(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train: 0.9428571428571428\n",
      "Score test:  1.0 \n",
      "\n",
      "Predictions train:\n",
      " [1,2,2,1,2,1,2,1,1,2,1,1,2,1,2,1,0,2,1,1,1,1,2,0,0,2,1,0,0,2,0,2,1,0,1,2,1,0,2,2,2,2,0,0,2,2,0,2,0,2,2,0,0,\n",
      " 2,0,0,0,1,2,2,0,0,0,1,1,0,0,1,0,2,1,2,1,0,2,0,2,0,0,2,0,2,1,1,1,2,2,1,2,0,1,2,2,0,1,1,2,1,0,0,0,2,1,2,0] \n",
      "\n",
      "Predictions test:\n",
      " [2,1,0,2,0,2,0,1,1,1,2,1,1,1,1,0,1,1,0,0,2,1,0,0,2,0,0,1,1,0,2,1,0,2,2,1,0,1,1,1,2,0,2,0,0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comparar con el modelo GaussianNB de scikit\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Calcular el porcentaje de acierto\n",
    "print(\"Score train:\", nb.score(X_train, y_train))\n",
    "print(\"Score test: \", nb.score(X_test, y_test), \"\\n\")\n",
    "\n",
    "# Predecir las etiquetas de los datos de entrenamiento. \n",
    "print(\"Predictions train:\\n\", np.array2string(nb.predict(X_train), separator=',', max_line_width=107), \"\\n\")\n",
    "print(\"Predictions test:\\n\", np.array2string(nb.predict(X_test), separator=',', max_line_width=107), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Y):\n",
      " [0.32380952 0.3047619  0.37142857] \n",
      "\n",
      "P(X0|Y):\n",
      "0:\tμ = 4.9941, σ² = 0.1258\n",
      "1:\tμ = 5.9219, σ² = 0.2811\n",
      "2:\tμ = 6.6538, σ² = 0.4261\n",
      "\n",
      "P(X1|Y):\n",
      "0:\tμ = 3.3824, σ² = 0.1532\n",
      "1:\tμ = 2.7563, σ² = 0.1156\n",
      "2:\tμ = 2.9872, σ² = 0.1201\n",
      "\n",
      "P(X2|Y):\n",
      "0:\tμ = 1.4529, σ² = 0.0201\n",
      "1:\tμ = 4.1969, σ² = 0.2341\n",
      "2:\tμ = 5.5974, σ² = 0.3141\n",
      "\n",
      "P(X3|Y):\n",
      "0:\tμ = 0.2324, σ² = 0.0087\n",
      "1:\tμ = 1.3062, σ² = 0.0418\n",
      "2:\tμ = 2.0308, σ² = 0.0673\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utils.see_probability_tables(nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Pruebas con la BBDD `adult`\n",
    "\n",
    "Ahora, vamos a utilizar la base de datos `adult` que hemos utilizado en la Práctica 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education-num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital-status         occupation   relationship   race     sex  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country income  \n",
       "0          2174             0              40  United-States  <=50K  \n",
       "1             0             0              13  United-States  <=50K  \n",
       "2             0             0              40  United-States  <=50K  \n",
       "3             0             0              40  United-States  <=50K  \n",
       "4             0             0              40           Cuba  <=50K  "
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_adult = pd.read_csv('adult.csv')\n",
    "df_adult_test = pd.read_csv('adult_test.csv')\n",
    "df_adult.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para simplificar, vamos a obtener en una sola celda varias combinaciones con las cuales probar el Naive Bayes:\n",
    "- `X_train`, `X_test` con todas las variables\n",
    "- `X_train_cont`, `X_test_cont` con solo las variables continuas\n",
    "- `X_train_cat`, `X_test_cat` con solo las variables categóricas\n",
    "- `X_train_onehot`, `X_test_onehot` con las variables numéricas, y las categóricas codificadas con one-hot (para poder entrenar por ejemplo el Naive Bayes gaussiano)\n",
    "- `X_train_disc`, `X_test_disc` con las variables numéricas, y las categóricas discretizadas (para poder entrenar por ejemplo el Naive Bayes categórico)\n",
    "- `y_train`, `y_test` con la variable objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables continuas, categóricas y objetivo\n",
    "cont_vars = [\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
    "cont_vars_idx = [df_adult.columns.get_loc(var) for var in cont_vars]\n",
    "\n",
    "cat_vars = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"]\n",
    "cat_vars_idx = [df_adult.columns.get_loc(var) for var in cat_vars]\n",
    "\n",
    "target_var = \"income\"\n",
    "\n",
    "# Número de bins para la discretización\n",
    "nBins = 5\n",
    "\n",
    "# X_train, X_test con todas las variables\n",
    "X_train = df_adult.drop(columns=[target_var]).values\n",
    "X_test = df_adult_test.drop(columns=[target_var]).values\n",
    "\n",
    "# Ordinal enconding de las categóricas para que no haya problemas con los strings. Ej: \"workclass\" -> \"State-gov\" -> 0, \"Self-emp-not-inc\" -> 1, ...\n",
    "enc = OrdinalEncoder()\n",
    "X_train[:, cat_vars_idx] = enc.fit_transform(X_train[:, cat_vars_idx])\n",
    "X_test[:, cat_vars_idx] = enc.transform(X_test[:, cat_vars_idx])\n",
    "\n",
    "# X_train_cont, X_test_cont con solo las variables continuas\n",
    "X_train_cont = X_train[:, cont_vars_idx]\n",
    "X_test_cont = X_test[:, cont_vars_idx]\n",
    "\n",
    "# X_train_cat, X_test_cat con solo las variables categóricas\n",
    "X_train_cat = X_train[:, cat_vars_idx]\n",
    "X_test_cat = X_test[:, cat_vars_idx]\n",
    "\n",
    "# X_train_onehot, X_test_onehot con las variables numéricas, y las categóricas codificadas con one-hot (simulamos todas numéricas)\n",
    "df_train_onehot = df_adult.drop(columns=[target_var])\n",
    "df_test_onehot = df_adult_test.drop(columns=[target_var])\n",
    "\n",
    "X_train_onehot = pd.get_dummies(df_train_onehot, columns=cat_vars).values\n",
    "X_test_onehot = pd.get_dummies(df_test_onehot, columns=cat_vars).values\n",
    "\n",
    "# X_train_disc, X_test_disc con las variables numéricas, y las categóricas discretizadas en nBins (simulamos todas categóricas)\n",
    "discretizer = KBinsDiscretizer(n_bins=nBins, encode='ordinal', strategy='uniform')\n",
    "X_train_disc = discretizer.fit_transform(X_train_cont)\n",
    "X_test_disc = discretizer.transform(X_test_cont)\n",
    "\n",
    "# y_train, y_test con la variable objetivo\n",
    "y_train = df_adult[target_var]\n",
    "y_test = df_adult_test[target_var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Prueba del NB mixto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing Feature 0 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = object, Sample = [39 50 38 53 28]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [39. 50. 38. 53. 28.]\n",
      "--- Feature 1 (Categorical): Appending column with Dtype = object ---\n",
      "--- Processing Feature 2 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = object, Sample = [77516 83311 215646 234721 338409]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [ 77516.  83311. 215646. 234721. 338409.]\n",
      "--- Feature 3 (Categorical): Appending column with Dtype = object ---\n",
      "--- Processing Feature 4 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = object, Sample = [13 13 9 7 13]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [13. 13.  9.  7. 13.]\n",
      "--- Feature 5 (Categorical): Appending column with Dtype = object ---\n",
      "--- Feature 6 (Categorical): Appending column with Dtype = object ---\n",
      "--- Feature 7 (Categorical): Appending column with Dtype = object ---\n",
      "--- Feature 8 (Categorical): Appending column with Dtype = object ---\n",
      "--- Feature 9 (Categorical): Appending column with Dtype = object ---\n",
      "--- Processing Feature 10 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = object, Sample = [2174 0 0 0 0]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [2174.    0.    0.    0.    0.]\n",
      "--- Processing Feature 11 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = object, Sample = [0 0 0 0 0]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [0. 0. 0. 0. 0.]\n",
      "--- Processing Feature 12 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = object, Sample = [40 13 40 40 40]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [40. 13. 40. 40. 40.]\n",
      "--- Feature 13 (Categorical): Appending column with Dtype = object ---\n",
      "\n",
      "--- Assembled X_processed ---\n",
      "Final X_processed Shape: (32561, 14)\n",
      "Final X_processed Dtype: object\n",
      "  Column 0 Final Dtype: object\n",
      "  Column 1 Final Dtype: object\n",
      "  Column 2 Final Dtype: object\n",
      "  Column 3 Final Dtype: object\n",
      "  Column 4 Final Dtype: object\n",
      "  Column 5 Final Dtype: object\n",
      "  Column 6 Final Dtype: object\n",
      "  Column 7 Final Dtype: object\n",
      "  Column 8 Final Dtype: object\n",
      "  Column 9 Final Dtype: object\n",
      "  Column 10 Final Dtype: object\n",
      "  Column 11 Final Dtype: object\n",
      "  Column 12 Final Dtype: object\n",
      "  Column 13 Final Dtype: object\n",
      "-----------------------------\n",
      "--- Processing Prediction Feature 0 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = object, Sample = [39 50 38 53 28]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [39. 50. 38. 53. 28.]\n",
      "--- Feature 1 (Categorical): Appending column with Dtype = object ---\n",
      "--- Processing Prediction Feature 2 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = object, Sample = [77516 83311 215646 234721 338409]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [ 77516.  83311. 215646. 234721. 338409.]\n",
      "--- Feature 3 (Categorical): Appending column with Dtype = object ---\n",
      "--- Processing Prediction Feature 4 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = object, Sample = [13 13 9 7 13]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [13. 13.  9.  7. 13.]\n",
      "--- Feature 5 (Categorical): Appending column with Dtype = object ---\n",
      "--- Feature 6 (Categorical): Appending column with Dtype = object ---\n",
      "--- Feature 7 (Categorical): Appending column with Dtype = object ---\n",
      "--- Feature 8 (Categorical): Appending column with Dtype = object ---\n",
      "--- Feature 9 (Categorical): Appending column with Dtype = object ---\n",
      "--- Processing Prediction Feature 10 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = object, Sample = [2174 0 0 0 0]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [2174.    0.    0.    0.    0.]\n",
      "--- Processing Prediction Feature 11 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = object, Sample = [0 0 0 0 0]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [0. 0. 0. 0. 0.]\n",
      "--- Processing Prediction Feature 12 (continuous) ---\n",
      "Before element-wise conversion (from original X): Dtype = object, Sample = [40 13 40 40 40]\n",
      "After element-wise conversion (processed column array): Dtype = float64, Sample = [40. 13. 40. 40. 40.]\n",
      "--- Feature 13 (Categorical): Appending column with Dtype = object ---\n",
      "\n",
      "--- Assembled X_processed (Prediction) ---\n",
      "Final X_processed Shape: (32561, 14)\n",
      "Final X_processed Dtype: object\n",
      "  Column 0 Final Dtype: object\n",
      "  Column 1 Final Dtype: object\n",
      "  Column 2 Final Dtype: object\n",
      "  Column 3 Final Dtype: object\n",
      "  Column 4 Final Dtype: object\n",
      "  Column 5 Final Dtype: object\n",
      "  Column 6 Final Dtype: object\n",
      "  Column 7 Final Dtype: object\n",
      "  Column 8 Final Dtype: object\n",
      "  Column 9 Final Dtype: object\n",
      "  Column 10 Final Dtype: object\n",
      "  Column 11 Final Dtype: object\n",
      "  Column 12 Final Dtype: object\n",
      "  Column 13 Final Dtype: object\n",
      "---------------------------------------\n"
     ]
    },
    {
     "ename": "UFuncTypeError",
     "evalue": "Cannot cast ufunc 'add' output from dtype('O') to dtype('float64') with casting rule 'same_kind'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[220], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m nb \u001b[38;5;241m=\u001b[39m NB(categorical_features\u001b[38;5;241m=\u001b[39mcat_vars_idx)\n\u001b[1;32m      3\u001b[0m nb\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScore train:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mScore test: \u001b[39m\u001b[38;5;124m\"\u001b[39m, nb\u001b[38;5;241m.\u001b[39mscore(X_test, y_test))\n",
      "Cell \u001b[0;32mIn[208], line 315\u001b[0m, in \u001b[0;36mNB.score\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mscore\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[1;32m    314\u001b[0m     correctos \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 315\u001b[0m     resultado \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X)):\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m  resultado[i] \u001b[38;5;241m==\u001b[39m y[i]:\n",
      "Cell \u001b[0;32mIn[208], line 309\u001b[0m, in \u001b[0;36mNB.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[208], line 297\u001b[0m, in \u001b[0;36mNB.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict_proba\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m--> 297\u001b[0m      prob_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m      p \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(prob_matrix, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    299\u001b[0m      \u001b[38;5;66;03m# Handle argmax of rows that might be all NaN or 0 after normalization (shouldn't happen with laplace + epsilon)\u001b[39;00m\n\u001b[1;32m    300\u001b[0m      \u001b[38;5;66;03m# If a row is all NaN, argmax raises ValueError. If all 0, argmax is 0.\u001b[39;00m\n\u001b[1;32m    301\u001b[0m      \u001b[38;5;66;03m# A robust argmax might handle NaN rows explicitly. For now, assume normalization prevents all NaNs if any original log_prob was finite.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[208], line 265\u001b[0m, in \u001b[0;36mNB.predict_proba_matrix\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    258\u001b[0m             log_likelihoods \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mpi \u001b[38;5;241m*\u001b[39m safe_var) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (diff\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m safe_var)\n\u001b[1;32m    260\u001b[0m             \u001b[38;5;66;03m# Handle cases where log_likelihoods might become NaN or Inf if input was NaN or calculation resulted in it\u001b[39;00m\n\u001b[1;32m    261\u001b[0m             \u001b[38;5;66;03m# NumPy arithmetic on NaN inputs usually results in NaN outputs, which is fine.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m             \u001b[38;5;66;03m# If diff**2 / safe_var results in Inf or a very large number, log_likelihoods can be -Inf.\u001b[39;00m\n\u001b[1;32m    263\u001b[0m             \u001b[38;5;66;03m# These extreme values are typically handled by log-sum-exp trick.\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m             \u001b[43mlog_probs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlog_likelihoods\u001b[49m \u001b[38;5;66;03m# This addition should work if log_likelihoods is float64\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Apply log-sum-exp trick and normalize\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# np.max and np.sum handle NaNs appropriately. max of [-inf, -inf] is -inf. sum involving NaN is NaN.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m max_log_probs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(log_probs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mUFuncTypeError\u001b[0m: Cannot cast ufunc 'add' output from dtype('O') to dtype('float64') with casting rule 'same_kind'"
     ]
    }
   ],
   "source": [
    "# Crear un modelo NB mixto\n",
    "nb = NB(categorical_features=cat_vars_idx)\n",
    "nb.fit(X_train, y_train)\n",
    "print(\"Score train:\", nb.score(X_train, y_train),\"\\nScore test: \", nb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Y):\n",
      " [0.75919044 0.24080956] \n",
      "\n",
      "P(X0|Y):\n",
      "<=50K:\tμ = 36.7837, σ² = 196.5629\n",
      ">50K:\tμ = 44.2498, σ² = 110.6499\n",
      "\n",
      "P(X1|Y):\n",
      "<=50K:\t[6.660e-02 2.390e-02 5.970e-02 3.000e-04 7.171e-01 2.000e-02 7.350e-02\n",
      " 3.830e-02 6.000e-04]\n",
      ">50K:\t[2.450e-02 4.740e-02 7.870e-02 1.000e-04 6.324e-01 7.940e-02 9.240e-02\n",
      " 4.510e-02 1.000e-04]\n",
      "\n",
      "P(X2|Y):\n",
      "<=50K:\tμ = 190340.8652, σ² = 11338474078.7776\n",
      ">50K:\tμ = 188005.0000, σ² = 10514815717.0130\n",
      "\n",
      "P(X3|Y):\n",
      "<=50K:\t[0.0353 0.0451 0.0162 0.0066 0.0129 0.0245 0.0197 0.0325 0.0413 0.1267\n",
      " 0.0044 0.3568 0.0309 0.0021 0.0062 0.2387]\n",
      ">50K:\t[8.000e-03 7.800e-03 4.300e-03 9.000e-04 2.200e-03 5.200e-03 3.600e-03\n",
      " 3.390e-02 4.610e-02 2.828e-01 3.910e-02 2.133e-01 1.222e-01 1.000e-04\n",
      " 5.400e-02 1.767e-01]\n",
      "\n",
      "P(X4|Y):\n",
      "<=50K:\tμ = 9.5951, σ² = 5.9348\n",
      ">50K:\tμ = 11.6117, σ² = 5.6888\n",
      "\n",
      "P(X5|Y):\n",
      "<=50K:\t[0.161  0.0006 0.3351 0.0156 0.4122 0.0388 0.0368]\n",
      ">50K:\t[0.0591 0.0014 0.8528 0.0045 0.0627 0.0085 0.011 ]\n",
      "\n",
      "P(X6|Y):\n",
      "<=50K:\t[0.0668 0.132  0.0004 0.1282 0.0849 0.0356 0.052  0.0709 0.1277 0.006\n",
      " 0.0923 0.0177 0.1079 0.0261 0.0517]\n",
      ">50K:\t[0.0244 0.0647 0.0003 0.1184 0.2506 0.0148 0.0111 0.032  0.0176 0.0003\n",
      " 0.2368 0.027  0.1253 0.0362 0.0409]\n",
      "\n",
      "P(X7|Y):\n",
      "<=50K:\t[0.2943 0.3013 0.0382 0.2023 0.1306 0.0333]\n",
      ">50K:\t[0.7543 0.1092 0.0048 0.0087 0.0279 0.0951]\n",
      "\n",
      "P(X8|Y):\n",
      "<=50K:\t[0.0112 0.0309 0.1107 0.01   0.8372]\n",
      ">50K:\t[0.0047 0.0353 0.0495 0.0033 0.9072]\n",
      "\n",
      "P(X9|Y):\n",
      "<=50K:\t[0.388 0.612]\n",
      ">50K:\t[0.1505 0.8495]\n",
      "\n",
      "P(X10|Y):\n",
      "<=50K:\tμ = 148.7525, σ² = 927637.3254\n",
      ">50K:\tμ = 4006.1425, σ² = 212295942.7839\n",
      "\n",
      "P(X11|Y):\n",
      "<=50K:\tμ = 53.1429, σ² = 96569.1480\n",
      ">50K:\tμ = 195.0015, σ² = 354605.4508\n",
      "\n",
      "P(X12|Y):\n",
      "<=50K:\tμ = 38.8402, σ² = 151.7576\n",
      ">50K:\tμ = 45.4730, σ² = 121.2855\n",
      "\n",
      "P(X13|Y):\n",
      "<=50K:\t[1.770e-02 5.000e-04 3.400e-03 2.300e-03 2.300e-03 2.900e-03 2.800e-03\n",
      " 1.000e-03 4.000e-03 2.500e-03 7.000e-04 3.800e-03 9.000e-04 2.500e-03\n",
      " 1.700e-03 1.000e-04 5.000e-04 6.000e-04 4.000e-04 2.500e-03 1.000e-03\n",
      " 8.000e-04 2.000e-03 2.900e-03 1.600e-03 7.000e-04 2.470e-02 1.300e-03\n",
      " 6.000e-04 1.200e-03 5.600e-03 2.000e-03 1.400e-03 4.200e-03 4.000e-04\n",
      " 2.600e-03 1.300e-03 6.000e-04 7.000e-04 8.885e-01 2.500e-03 4.000e-04]\n",
      ">50K:\t[1.860e-02 1.000e-03 5.100e-03 2.700e-03 4.000e-04 3.300e-03 4.000e-04\n",
      " 6.000e-04 1.300e-03 3.900e-03 1.600e-03 5.700e-03 1.100e-03 5.000e-04\n",
      " 6.000e-04 1.000e-04 3.000e-04 9.000e-04 5.000e-04 5.200e-03 2.400e-03\n",
      " 8.000e-04 3.300e-03 1.400e-03 3.200e-03 4.000e-04 4.300e-03 4.000e-04\n",
      " 1.000e-04 4.000e-04 7.900e-03 1.600e-03 6.000e-04 1.600e-03 5.000e-04\n",
      " 2.200e-03 2.700e-03 5.000e-04 4.000e-04 9.098e-01 8.000e-04 9.000e-04]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utils.see_probability_tables(nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 `TODO:` Experimentación exhaustiva\n",
    "\n",
    "En esta sección, se propone realizar una experimentación exhaustiva con el modelo `NB` implementado, comparándolo con los modelos de `scikit-learn` y con los árboles de decisión estudiados en la Práctica 1. El objetivo es entender el comportamiento del Naive Bayes mixto en diferentes escenarios y sacar conclusiones sobre su rendimiento. \n",
    "\n",
    "Para ello, se puede probar el modelo `NB` en diferentes configuraciones: solo variables categóricas (`X_train_cat`), solo variables numéricas (`X_train_cont`), una combinación de ambas (`X_train`), discretizando las numéricas (`X_train_disc`) o aplicando un one-hot a las categóricas (`X_train_onehot`).\n",
    "\n",
    "Obtén métricas como la tasa de acierto (en train y test) o el tiempo de ejecución. Además, también se pueden obtener métricas adicionales como precisión, recall o F1-score, ya que la base de datos está desbalanceada. Compara los resultados con los obtenidos en la Práctica 1 y analiza qué modelo funciona mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para empezar con la experimentación, vamos a preparar las bases de datos para poder experimentar con ellas (solo se ha tenido que preparar las bases de datos para los arbols de decisión)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_all = cont_vars + cat_vars\n",
    "vars_all_idx = cont_vars_idx + cat_vars_idx\n",
    "vars_all_mixto = cont_vars + cat_vars\n",
    "cont_vars_idx_mixto = list(range(len(cont_vars)))\n",
    "cat_vars_idx_mixto = list(range(len(cont_vars), len(cont_vars) + len(cat_vars)))\n",
    "\n",
    "\n",
    "vars_all_cat = cat_vars\n",
    "cont_vars_idx_cat = []\n",
    "cat_vars_idx_cat = list(range(len(cat_vars)))\n",
    "\n",
    "\n",
    "vars_all_cont = cont_vars\n",
    "cont_vars_idx_cont = list(range(len(cont_vars)))\n",
    "cat_vars_idx_cont = []\n",
    "\n",
    "\n",
    "df_train_onehot_temp = df_adult.drop(columns=[target_var])\n",
    "X_train_onehot_cols = pd.get_dummies(df_train_onehot_temp, columns=cat_vars).columns.tolist()\n",
    "vars_all_onehot = X_train_onehot_cols\n",
    "\n",
    "cont_vars_idx_onehot = list(range(len(cont_vars)))\n",
    "cat_vars_idx_onehot = list(range(len(cont_vars), len(vars_all_onehot)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente bloque, hemos añadido un diccionario con el modelo, que tipo de modelo y que datos se van a usar. Gracias a esto podemos probar, como se indica en el enunciado, solo variables categoricas, solo numericas, combinación, discretizando y one-hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilsP1 as p1\n",
    "\n",
    "naiveBayess = {\n",
    "    \"NB_mix\": (NB(categorical_features=cat_vars_idx), X_train, X_test),\n",
    "    \"NB_cat\": (NB(categorical_features='all'), X_train_cat, X_test_cat),\n",
    "    \"NB_onehot\": (NB(categorical_features='none'), X_train_onehot, X_test_onehot),\n",
    "    \"NB_cont\": (NB(categorical_features='none'),  X_train_cont, X_test_cont),\n",
    "    \"Sklearn_mix\": (GaussianNB(), X_train, X_test),\n",
    "    \"Sklearn_Cat\": (CategoricalNB(), X_train_cat, X_test_cat),\n",
    "    \"Sklearn_onehot\": (GaussianNB(), X_train_onehot, X_test_onehot),\n",
    "    \"Sklearn_cont\": (GaussianNB(), X_train_cont, X_test_cont)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "arboless = {\n",
    "    \"Arbol_mixto\": (p1.C45Classifier(vars_all_mixto, cat_vars_idx_mixto, cont_vars_idx_mixto, max_depth=25, prune=False), X_train, X_test),\n",
    "    \"Arbol_cat\": (p1.C45Classifier(vars_all_cat, cat_vars_idx_cat, cont_vars_idx_cat, max_depth=25, prune=False), X_train_cat, X_test_cat),\n",
    "    \"Arbol_onehot\": (p1.C45Classifier(vars_all_onehot, cat_vars_idx_onehot, cont_vars_idx_onehot, max_depth=25, prune=False),  X_train_onehot, X_test_onehot),\n",
    "    \"Arbol_cont\": (p1.C45Classifier(vars_all_cont, cat_vars_idx_cont, cont_vars_idx_cont, max_depth=25, prune=False), X_train_cont, X_test_cont),\n",
    "    \"Arbol_mixto_prune\": (p1.C45Classifier(vars_all_mixto, cat_vars_idx_mixto, cont_vars_idx_mixto, max_depth=25, prune=True), X_train, X_test),\n",
    "    \"Arbol_cat_prune\": (p1.C45Classifier(vars_all_cat, cat_vars_idx_cat, cont_vars_idx_cat, max_depth=25, prune=True), X_train_cat, X_test_cat),\n",
    "    \"Arbol_onehot_prune\": (p1.C45Classifier(vars_all_onehot, cat_vars_idx_onehot, cont_vars_idx_onehot, max_depth=25, prune=True),  X_train_onehot, X_test_onehot),\n",
    "    \"Arbol_cont_prune\": (p1.C45Classifier(vars_all_cont, cat_vars_idx_cont, cont_vars_idx_cont, max_depth=25, prune=True), X_train_cont, X_test_cont)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el script que corre todas las configuraciones superiores. Se ha realizado de esta manera para hacer mas facil añadir casos especificos. El script termina almacenando los resultados de la ejeccucion de los mismos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32524/521457384.py:120: RuntimeWarning: divide by zero encountered in log\n",
      "  log_prob_clss += -0.5 * np.log(2 * np.pi * var) - 0.5 * ((x[j] - mean)**2 / var) # Forma simplificada que aumenta el score (por algún motivo)\n",
      "/tmp/ipykernel_32524/521457384.py:120: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  log_prob_clss += -0.5 * np.log(2 * np.pi * var) - 0.5 * ((x[j] - mean)**2 / var) # Forma simplificada que aumenta el score (por algún motivo)\n",
      "/tmp/ipykernel_32524/521457384.py:120: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  log_prob_clss += -0.5 * np.log(2 * np.pi * var) - 0.5 * ((x[j] - mean)**2 / var) # Forma simplificada que aumenta el score (por algún motivo)\n",
      "/tmp/ipykernel_32524/521457384.py:120: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  log_prob_clss += -0.5 * np.log(2 * np.pi * var) - 0.5 * ((x[j] - mean)**2 / var) # Forma simplificada que aumenta el score (por algún motivo)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "results = [] # Para almacenar los resultados\n",
    "\n",
    "# Bloque para los modelos de Bayes de la practica 2\n",
    "for name, (model, X_tr, X_te) in naiveBayess.items():\n",
    "    ini = time.time()\n",
    "    model.fit(X_tr, y_train)\n",
    "    train_time = time.time() - ini\n",
    "\n",
    "    \n",
    "    acc_train = model.score(X_tr,y_train)\n",
    "\n",
    "    if name == \"Sklearn_onehot\":\n",
    "        if X_te.shape[1] != X_train_onehot.shape[1]:\n",
    "            X_te = np.pad(X_te, ((0, 0), (0, X_train_onehot.shape[1] - X_te.shape[1])), mode='constant')\n",
    "\n",
    "    \n",
    "    ini = time.time()\n",
    "    y_pred_test = model.predict(X_te)\n",
    "    predict_time = time.time() - ini\n",
    "\n",
    "\n",
    "    acc_test = model.score(X_te, y_test)\n",
    "\n",
    "    y_pred_train = model.predict(X_tr)\n",
    "    f1_test = f1_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
    "\n",
    "    report = classification_report(y_test, y_pred_test, zero_division=0)\n",
    "\n",
    "    results.append({\n",
    "        \"Modelo\": name,\n",
    "        \"Precisión (Train)\": acc_train,\n",
    "        \"Precisión (Test)\": acc_test,\n",
    "        \"F1 (Test)\": f1_test,\n",
    "        \"Train tiempo (s)\": train_time,\n",
    "        \"Predict tiempo (s)\": predict_time,\n",
    "        \"Classification\": report \n",
    "    })\n",
    "\n",
    "# Bloque para arbol de decisión de la practica 1\n",
    "for name, (model, X_tr, X_te) in arboless.items():\n",
    "    ini = time.time()\n",
    "    model.fit(X_tr, y_train)\n",
    "    train_time = time.time() - ini\n",
    "\n",
    "    acc_train = model.score(X_tr, y_train)\n",
    "    acc_test = model.score(X_te, y_test)\n",
    "\n",
    "    ini = time.time()    \n",
    "    y_pred_test = model.predict(X_te)\n",
    "    predict_time = time.time() - ini\n",
    "    \n",
    "    f1_test = f1_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
    "\n",
    "    results.append({\n",
    "        \"Modelo\": name,\n",
    "        \"Precisión (Train)\": acc_train,\n",
    "        \"Precisión (Test)\": acc_test,\n",
    "        \"F1 (Test)\": f1_test,\n",
    "        \"Train tiempo (s)\": train_time,\n",
    "        \"Predict tiempo (s)\": predict_time,\n",
    "        \"Classification\": None \n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comparación ---\n",
      "            Modelo  Precisión (Train)  Precisión (Test)  F1 (Test)  Train tiempo (s)  Predict tiempo (s)\n",
      "            NB_mix           0.833727          0.831153   0.821903          0.522621            0.830883\n",
      "            NB_cat           0.795829          0.800872   0.809293          0.451901            0.583295\n",
      "         NB_onehot           0.759190          0.763774   0.661480          1.005999            5.021127\n",
      "           NB_cont           0.796229          0.796143   0.767227          0.063728            0.428957\n",
      "       Sklearn_mix           0.795031          0.795344   0.766881          0.033258            0.007321\n",
      "       Sklearn_Cat           0.795829          0.800872   0.809293          0.044584            0.004694\n",
      "    Sklearn_onehot           0.795092          0.795774   0.767058          0.148723            0.056306\n",
      "      Sklearn_cont           0.795092          0.795590   0.766827          0.024239            0.003057\n",
      "       Arbol_mixto           0.759190          0.763774   0.661480          0.225736            0.002916\n",
      "         Arbol_cat           0.759190          0.763774   0.661480          0.141651            0.002885\n",
      "      Arbol_onehot           0.759190          0.763774   0.661480          1.584923            0.003161\n",
      "        Arbol_cont           0.759190          0.763774   0.661480          0.109685            0.002953\n",
      " Arbol_mixto_prune           0.759190          0.763774   0.661480          0.236030            0.002919\n",
      "   Arbol_cat_prune           0.759190          0.763774   0.661480          0.137605            0.002966\n",
      "Arbol_onehot_prune           0.759190          0.763774   0.661480          1.528829            0.002937\n",
      "  Arbol_cont_prune           0.759190          0.763774   0.661480          0.104465            0.002937\n",
      "\n",
      "--- Classification ---\n",
      "\n",
      "Modelo: NB_mix\n",
      "--------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.86      0.93      0.89     12435\n",
      "        >50K       0.69      0.51      0.59      3846\n",
      "\n",
      "    accuracy                           0.83     16281\n",
      "   macro avg       0.78      0.72      0.74     16281\n",
      "weighted avg       0.82      0.83      0.82     16281\n",
      "\n",
      "\n",
      "Modelo: NB_cat\n",
      "--------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.91      0.82      0.86     12435\n",
      "        >50K       0.56      0.74      0.64      3846\n",
      "\n",
      "    accuracy                           0.80     16281\n",
      "   macro avg       0.73      0.78      0.75     16281\n",
      "weighted avg       0.83      0.80      0.81     16281\n",
      "\n",
      "\n",
      "Modelo: NB_onehot\n",
      "-----------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.76      1.00      0.87     12435\n",
      "        >50K       0.00      0.00      0.00      3846\n",
      "\n",
      "    accuracy                           0.76     16281\n",
      "   macro avg       0.38      0.50      0.43     16281\n",
      "weighted avg       0.58      0.76      0.66     16281\n",
      "\n",
      "\n",
      "Modelo: NB_cont\n",
      "---------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.82      0.95      0.88     12435\n",
      "        >50K       0.65      0.30      0.41      3846\n",
      "\n",
      "    accuracy                           0.80     16281\n",
      "   macro avg       0.73      0.63      0.65     16281\n",
      "weighted avg       0.77      0.80      0.77     16281\n",
      "\n",
      "\n",
      "Modelo: Sklearn_mix\n",
      "-------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.82      0.95      0.88     12435\n",
      "        >50K       0.64      0.31      0.41      3846\n",
      "\n",
      "    accuracy                           0.80     16281\n",
      "   macro avg       0.73      0.63      0.65     16281\n",
      "weighted avg       0.77      0.80      0.77     16281\n",
      "\n",
      "\n",
      "Modelo: Sklearn_Cat\n",
      "-------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.91      0.82      0.86     12435\n",
      "        >50K       0.56      0.74      0.64      3846\n",
      "\n",
      "    accuracy                           0.80     16281\n",
      "   macro avg       0.73      0.78      0.75     16281\n",
      "weighted avg       0.83      0.80      0.81     16281\n",
      "\n",
      "\n",
      "Modelo: Sklearn_onehot\n",
      "----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.82      0.95      0.88     12435\n",
      "        >50K       0.64      0.30      0.41      3846\n",
      "\n",
      "    accuracy                           0.80     16281\n",
      "   macro avg       0.73      0.63      0.65     16281\n",
      "weighted avg       0.77      0.80      0.77     16281\n",
      "\n",
      "\n",
      "Modelo: Sklearn_cont\n",
      "--------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.81      0.95      0.88     12435\n",
      "        >50K       0.64      0.30      0.41      3846\n",
      "\n",
      "    accuracy                           0.80     16281\n",
      "   macro avg       0.73      0.63      0.64     16281\n",
      "weighted avg       0.77      0.80      0.77     16281\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Comparación ---\")\n",
    "# Mostramos las columnas clave de forma tabular\n",
    "print(results_df.drop(columns=[\"Classification\"]).to_string(index=False))\n",
    "\n",
    "print(\"\\n--- Classification ---\\n\")\n",
    "for idx, row in results_df.iterrows():\n",
    "    if row[\"Classification\"] is not None:\n",
    "        print(f\"Modelo: {row['Modelo']}\")\n",
    "        print(\"-\" * (8 + len(row['Modelo'])))\n",
    "        print(row[\"Classification\"])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos las métricas, podemos empezar a comparar. Antes de eso, aclarar que el tiempo variará un poco según el ordenador donde se ejecute, pero el reporte ha sido realizado teniendo en cuenta las métricas aparecidas en nuestro caso. La tabla de resultados incluye modelos Naive Bayes (`NB` y `Sklearn`) y modelos de Árbol de Decisión (`Arbol`).\n",
    "\n",
    "#### Comparación de Modelos\n",
    "\n",
    "##### Naive Bayes\n",
    "\n",
    "Analizamos el rendimiento de nuestra implementación de Naive Bayes (`NB`) frente a la de scikit-learn (`Sklearn`) en diferentes escenarios de datos:\n",
    "\n",
    "* **Solo variables categóricas:** Tanto nuestra implementación (`NB_cat`) frente a la de scikit-learn (`Sklearn_Cat`) obtiene un rendimiento casi indéntico (Precisión Test ~80.09%, F1 Test ~0.809), confirmando que nuestra implementación funciona de forma correcta para las variables categóricas. Sin embargo, la diferencia en tiempos de ejecución es notable, siendo nuestra implementación significativamente más lenta en entrenamiento (0.039 s vs 0.307 s) y predicción (0.004 s vs 0.399 s).\n",
    "* **Solo variables numéricas:** Nuestra implementación (`NB_cont`) frente a la de scikit-learn (`Sklearn_cont`) obtiene  rendimientos predictivos muy similares (Precisión Test ~79.61% vs 79.56%, F1 Test ~0.767 vs 0.767). Nuestra implementación fue ligeramente más lenta entrenando (0.046 s vs 0.020 s), pero mucho más lenta prediciendo (0.294 s vs 0.003 s).\n",
    "* **Combinación categóricas y numéricas (Mixto):** Aquí es donde nuestra implementación (`NB_mix`) destacó en rendimiento predictivo, superando a `Sklearn_mix` (versión gaussiana de sklearn) con una Precisión Test del 83.12% frente al 79.53% y un F1 Test del 0.822 frente al 0.767. Esta mejora viene acompañada de tiempos de ejecución más altos (Entrenamiento 0.347 s vs 0.024 s, Predicción 0.573 s vs 0.005 s).\n",
    "* **Datos con One-hot Encoding:** El uso de one-hot encoding en las variables categóricas, (`NB_onehot` y `Sklearn_onehot`), resultó en un rendimiento generalmente inferior. Nuestra implementación `NB_onehot` fue la que peor desempeño tuvo (Precisión Test 76.38%, F1 Test 0.661), mostrando en el reporte de clasificación una clara tendencia a predecir solo la clase mayoritaria. `Sklearn_onehot` se comportó de forma similar a los otros modelos gaussianos de sklearn (Precisión Test 79.58%, F1 Test 0.767), mejor que el nuestro.\n",
    "\n",
    "##### Árbol de Decisión\n",
    "\n",
    "Hemos incluido en la comparación diferentes configuraciones de nuestro implementado Árbol de Decisión (C4.5), variando el tipo de datos de entrada (mixto, categórico, one-hot, continuo) y aplicando o no poda. Un aspecto notable de los resultados de esta ejecución es que **todas** las configuraciones de Árbol de Decisión obtuvieron exactamente las mismas métricas de rendimiento predictivo: Precisión en Test del 76.38% y F1 Score ponderado del 0.661. Teniendo en cuenta que hemos usado la misma profundidad máxima y el mismo criterio, ambos los encontrados en el estudio de la practica pasada que resultaban en mejor rendimiento, es normal que se comporte de manera similar.\n",
    "\n",
    "En esta comparación, mientras que los Árboles de Decisión destacaron por su velocidad, no lograron alcanzar el mismo nivel de precisión y F1 Score que los modelos Naive Bayes más efectivos para este conjunto de datos, particularmente `NB_mix`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "\n",
    "Gracias a esta practica, nuestro conocimiento sobre el modelo naive bayes ha aumentado, lo que nos ayudará a en futuros ejercicios, examenes o incluso aplicaciones reales. Lo más complicado de esta practica ha sido estructurar bien todas las tablas y sobre todo hacer que las probabilidades disesen de forma correcta. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Practica 1 - Arboles de decision.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "VirtualEnvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
