{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "V01Ub14OGcRJ"
   },
   "source": [
    "<br><br><br>\n",
    "<h2><font size=6>Práctica 2</font></h2>\n",
    "\n",
    "\n",
    "\n",
    "<h1><font size=7>Naive Bayes</font></h1>\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: right\">\n",
    "<font size=4>Diego García Díaz (Diego.Garcia30@alu.uclm.es)</font><br>\n",
    "<font size=4>Alberto Pérez Álvarez (Alberto.Perez25@alu.uclm.es)</font><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"#B30033\" size=5>Estudiantes: </font>** \n",
    "\n",
    "* Diego García Díaz\n",
    "* Alberto Pérez Álvarez "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "B-Yy-FwTG33Y"
   },
   "source": [
    "---\n",
    "\n",
    "<a id=\"introduccion\"></a>\n",
    "# 1. Introducción\n",
    "\n",
    "<br>\n",
    "\n",
    "El objetivo de esta práctica es estudiar el algoritmo [`Naive Bayes`](https://scikit-learn.org/stable/modules/naive_bayes.html) para clasificación. Para ello, se deberá:\n",
    "1. Llevar a cabo un estudio de los clasificadores ya implementados de la librería `scikit-learn`. En él, se estudiará tanto la versión de Naive Bayes para variables discretas, [`CategoricalNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html), como la versión Gaussiana para variables continuas, [`GaussianNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html). En el caso de `CategoricalNB`, además, se estudiará el efecto de la discretización de variables sobre los resultados.\n",
    "2. Implementación del algoritmo Naive Bayes mixto (que pueda tratar tanto variables discretas como continuas) en Python y realización de una comparación con los utilizados de `scikit-learn`.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5vR6t4WXGpmb"
   },
   "source": [
    "---\n",
    "\n",
    "<a id=\"indice\"></a>\n",
    "<h2>Índice</h2>\n",
    "\n",
    "* [1. Introducción](#introduccion)\n",
    "* [2. Modelos Gráficos Probabilísticos](#pgm)\n",
    "* [3. Naive Bayes en scikit-learn](#naive-scikit)\n",
    "* [4. Implementación de Naive Bayes categórico](#implementacion)\n",
    "* [5. Evaluación experimental](#evaluacion)\n",
    "* [6. Trabajo opcional](#opcional)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"pgm\"></a>\n",
    "# 2. Modelos Gráficos Probabilísticos\n",
    "\n",
    "<br>\n",
    "\n",
    "## 2.1 Clasificación probabilística\n",
    "\n",
    "Como ya habéis visto en clase, podemos realizar una predicción de las probabilidades a posteriori de los posibles valores $\\{c_1,\\dots,c_k\\}$ de una variable $y$ mediante el cálculo de $P(y\\ |\\ x_1,\\dots,x_n)$, donde $x_1,\\dots,x_n$ son las $n$ variables independientes usadas para estimar $y$. Una vez contamos con las probabilidades, si queremos realizar una clasificación simplemente habrá que devolver el valor $\\{c_1,\\dots,c_k\\}$ con mayor probabilidad, es decir, $\\hat{y} = arg\\ max_{y \\in \\{c_1,\\dots,c_k\\}} P(y\\ |\\ x_1,\\dots,x_n)$.\n",
    "\n",
    "\n",
    "En la práctica, es imposible realizar dicho cálculo en la mayoría de ocasiones ya que aún contando con variables binarias se necesitaría crear una **Distribución de Probabilidad Conjunta (DCP)** en la que se almacene la probabilidad de cada combinación, es decir, $2^n$ valores.\n",
    "\n",
    "Podemos intentar reducir el tamaño de la DCP. Usando el **Teorema de Bayes**, tenemos que: $$P(y\\ |\\ x_1,\\dots,x_n) = \\frac{P(y)P(x_1,\\dots,x_n\\ |\\ y)}{P(x_1,\\dots,x_n)} = \\frac{P(y,x_1,\\dots,x_n)}{P(x_1,\\dots,x_n)}$$\n",
    "\n",
    "Ya que el denomiador va a ser constante independiententemente de $c$, lo podemos eliminar, quedándonos lo siguiente: $$P(y\\ |\\ x_1,\\dots,x_n) \\propto P(y)P(x_1,\\dots,x_n\\ |\\ y) = P(y,x_1,\\dots,x_n)$$\n",
    "\n",
    "Esta conversión es fundamental para las **Redes Bayesianas**, ya que gracias a ella podemos aplicar la **regla de la cadena** y la **Condición de Markov** para poder almacenar eficientemente la Distribución de Probabilidad Conjunta. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "kiYVXSxiYNNw"
   },
   "source": [
    "## 2.2 Redes Bayesianas\n",
    "\n",
    "<img src=\"Imagenes/BN.png\" width=\"600\">\n",
    "\n",
    "Pese a la reducción considerable en cuanto a complejidad que conseguimos con los métodos comentados anteriormente, el aprendizaje de una Red Bayesiana sigue siendo un problema NP-Hard. Este aprendizaje se divide en dos fases:\n",
    "\n",
    "- **Aprendizaje estructural**: Obtener la **estructura gráfica** de la Red Bayesiana:\n",
    "\n",
    "> En la práctica, suele ser la parte más compleja. Se suelen utilizar algoritmos voraces en los que se limita lo máximo posible el espacio de búsqueda de los enlaces a añadir. Por ejemplo, en una Red Bayesiana con 1000 variables, para obtener el resultado óptimo habría que probar todas las combinaciones posibles de los aproximadamente 1.000.000 enlaces que se pueden añadir.\n",
    "- **Aprendizaje paramétrico**: Obtener las **tablas de probabilidades** asociadas a cada variable:\n",
    "\n",
    "> Su complejidad se reduce mucho gracias a la factorización, ya que para cada variable solo hay que añadir a dichas tablas sus variables padres. Es decir, para la Red Bayesiana de ejemplo de la imagen superior, para la variable $X_6$ será necesario almacenar $P(X_6,X_2,X_3$); para $X_7$, $P(X_7,X_1)$; y para $X_1$, $P(X_1)$. En este ejemplo dichas tablas son pequeñas, pero sin embargo, si en nuestra red está densamente conectada (es decir, cada variable tiene un gran número de padres), la complejidad tanto temporal como espacial de esta fase del aprendizaje se puede disparar, ya que para variables binarias la tabla asociada a cada variable tendrá $2^n$ valores (si las variables en vez de binarias tuviesen por ejemplo 3 valores, sería $3^n$, y así sucesivamente).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.3 Naive Bayes\n",
    "\n",
    "<img src=\"Imagenes/Naive.png\" width=\"800\">\n",
    "\n",
    "**Naive Bayes** es un algoritmo de clasificación probabilística que permite solucionar los problemas en cuanto a la complejidad de las Redes Bayesianas, ya que por un lado al contar con una **estructura fija** no es necesario realizar **aprendizaje estructural**, y por otro cada variable únicamente va a tener un padre (la variable clase), por lo que su **aprendizaje paramétrico** también es muy simple.\n",
    "\n",
    "Así, se factoriza la Distribución de Probabilidad Conjunta de tal modo que finalmente contamos con: $$P(y\\ |\\ x_1,\\dots,x_n) \\propto P(y)\\prod_{i=1}^{n}P(x_i\\ |\\ y)$$ \n",
    "\n",
    "Por lo que podremos realizar predicciones con: $$\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n}P(x_i\\ |\\ y)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.4 Estimación de las probabilidades\n",
    "\n",
    "Por tanto, necesitamos estimar las distribuciones de probabilidad condicional $P(x_i\\ |\\ y)$ para cada variable $x_i$​ dado el valor de la clase $y$, además de la distribución de la clase $P(y)$. Esto se hace de manera diferente para variables discretas y continuas:\n",
    "\n",
    "* **Variables discretas:** Las podemos estimar con las frecuencias relativas del conjunto de entrenamiento. Así, podemos calcular $P(x_i\\ |\\ y)$ contando las frecuencias de cada valor de $x_i$ para cada valor de la clase $y$ en nuestros datos, y luego normalizándolos para obtener probabilidades.\n",
    "\n",
    "* **Variables continuas:** Para estas variables generalmente se asume una distribución de probabilidad específica, como la distribución normal (Gaussiana). Entonces, para cada variable continua $x_i$, estimamos los parámetros de esta distribución (como pueden ser la media y desviación estándar) utilizando los datos de entrenamiento correspondientes a cada valor de la clase $y$. Una vez que se han estimado estos parámetros, podemos usarlos para calcular la probabilidad condicional $P(x_i\\ |\\ y)$ para cada valor de $y$ con una nueva observación $x_i$​."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"naive-scikit\"></a>\n",
    "# 3 [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html) en `scikit-learn`\n",
    "\n",
    "\n",
    "`scikit-learn` cuenta con cinco implementaciones distintas de Naive Bayes: [`CategoricalNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html), [`GaussianNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html), [`BernoulliNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html), [`MultinomialNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) y [`ComplementNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.ComplementNB.html). En nuestro caso, nos vamos a centrar en `CategoricalNB` y `GaussianNB`, las dos versiones más utilizadas, y que están relacionadas con el código que se deberá implementar (el resto se utilizan principalmente para predicción en textos).\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## 3.1 `CategoricalNB`\n",
    "\n",
    "`CategoricalNB` es la implementación de Naive Bayes para variables discretas, explicada en la Sección 2.3 y que tendréis que implementar en la Sección 4. Siguiendo la estructura típica de `scikit-learn`, cuenta con un método `fit(X,y)` para entrenar el modelo y un `predict(X)` para realizar las predicciones. Además, caben destacar otros métodos como `partial_fit(X,y)`, para añadir datos a un modelo ya entrenado al ser un modelo incremental; `predict_proba(X)`, para predecir la probabilidades de cada clase para cada instancia de $X$ en lugar de únicamente obtener la clase predicha; y `score(X,y)`, que devuelve el porcentaje de acierto del modelo dadas unas instancias $X$ y las clases reales $y$.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a probar algunos de estos métodos con la base de datos `iris` (la podemos cargar directamente desde `scikit-learn`). Primero importamos todo lo necesario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WJlZrshbbZvc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "import utilsP2 as utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero vamos a cargar la base de datos a un dataframe de Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el conjunto de datos desde scikit-learn\n",
    "iris = load_iris()\n",
    "\n",
    "# Crear un dataframe de Pandas con los datos y las etiquetas\n",
    "df_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df_iris['target'] = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_iris.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y vamos a dividir los datos en entrenamiento ($X_{train}$ e $y_{train}$) y test ($X_{test}$ e $y_{test}$). Esto no fue necesario en la Práctica 1 ya que el conjunto `adult` nos proporcionaba un `.csv` distinto para entrenamiento y test.\n",
    "\n",
    "Mostramos las 5 primeras instancias de los conjuntos de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5. , 2. , 3.5, 1. ],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.7, 2.5, 5.8, 1.8]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = utils.train_test(df_iris)\n",
    "\n",
    "X_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como estamos probando `CategoricalNB` y nuestras variables son continuas, vamos a discretizarlas con `KBinsDiscretizer` antes de crear el modelo. En este caso, hemos elegido un discretizado uniforme en 3 bins, por lo que si aplicamos el discretizado todos los valores de las variables serán 0, 1 o 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 1.],\n",
       "       [1., 1., 2., 2.],\n",
       "       [2., 1., 2., 2.],\n",
       "       [1., 0., 2., 1.],\n",
       "       [2., 0., 2., 2.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Discretizar X \n",
    "est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
    "\n",
    "X_train_disc = est.fit_transform(X_train)\n",
    "X_test_disc = est.transform(X_test)\n",
    "X_train_disc[0:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora sí, entrenamos nuestro `CategoricalNB` con `fit(X)`, y calculamos el score del algoritmo con `score(X,y)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentaje de acierto en entrenamiento:  0.9428571428571428\n",
      "Porcentaje de acierto en prueba:  0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Crear un modelo NB\n",
    "categoricalNB = CategoricalNB()\n",
    " \n",
    "# Entrenar el modelo\n",
    "categoricalNB.fit(X_train_disc, y_train)\n",
    "\n",
    "# Calcular el porcentaje de acierto\n",
    "scoreTrain = categoricalNB.score(X_train_disc, y_train)\n",
    "scoreTest = categoricalNB.score(X_test_disc, y_test)\n",
    "print(\"Porcentaje de acierto en entrenamiento: \", scoreTrain)\n",
    "print(\"Porcentaje de acierto en prueba: \", scoreTest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar `predict(X)` para ver las predicciones realizadas para cada instancia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions Train:\n",
      " [1,2,2,1,2,1,2,1,1,1,1,1,2,1,2,1,0,2,1,1,1,1,2,0,0,2,1,0,0,1,0,2,1,0,1,2,1,0,2,2,2,2,0,0,2,2,0,2,0,2,2,0,0,\n",
      " 2,0,0,0,1,2,2,0,0,0,1,1,0,0,1,0,2,1,2,1,0,2,0,2,0,0,2,0,2,1,1,1,2,2,1,2,0,1,2,2,0,1,1,2,1,0,0,0,2,1,2,0] \n",
      "\n",
      "Predictions Test:\n",
      " [2,1,0,2,0,2,0,1,1,1,1,1,1,1,1,0,1,1,0,0,1,1,0,0,1,0,0,1,1,0,2,1,0,2,2,1,0,1,1,1,2,0,2,0,0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predecir las etiquetas de los datos de entrenamiento. \n",
    "predictionsTrain = categoricalNB.predict(X_train_disc)\n",
    "print(\"Predictions Train:\\n\", np.array2string(predictionsTrain, separator=',', max_line_width=107), \"\\n\")\n",
    "\n",
    "# Predecir las etiquetas de los datos de prueba. Mostrar 25 números por linea\n",
    "predictionsTest = categoricalNB.predict(X_test_disc)\n",
    "print(\"Predictions Test:\\n\", np.array2string(predictionsTest, separator=',', max_line_width=107), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y los podríamos comparar con los valores reales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train:\n",
      " [1,2,2,2,2,1,2,1,1,2,2,2,2,1,2,1,0,2,1,1,1,1,2,0,0,2,1,0,0,1,0,2,1,0,1,2,1,0,2,2,2,2,0,0,2,2,0,2,0,2,2,0,0,\n",
      " 2,0,0,0,1,2,2,0,0,0,1,1,0,0,1,0,2,1,2,1,0,2,0,2,0,0,2,0,2,1,1,1,2,2,1,1,0,1,2,2,0,1,1,1,1,0,0,0,2,1,2,0] \n",
      "\n",
      "y_test:\n",
      " [2,1,0,2,0,2,0,1,1,1,2,1,1,1,1,0,1,1,0,0,2,1,0,0,2,0,0,1,1,0,2,1,0,2,2,1,0,1,1,1,2,0,2,0,0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"y_train:\\n\", np.array2string(y_train, separator=',', max_line_width=107), \"\\n\")\n",
    "print(\"y_test:\\n\", np.array2string(y_test, separator=',', max_line_width=107), \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y también podemos ver las probalidades predichas para cada instancia con `predict_proba(X)` (mostramos solo las 5 primeras intancias):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.83088646e-04, 9.95159326e-01, 4.05758490e-03],\n",
       "       [2.27560421e-04, 6.41968548e-03, 9.93352754e-01],\n",
       "       [5.71964381e-05, 1.24120253e-03, 9.98701601e-01],\n",
       "       [1.23476706e-04, 5.26427262e-01, 4.73449262e-01],\n",
       "       [7.03293093e-06, 2.23206016e-03, 9.97760907e-01]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categoricalNB.predict_proba(X_train_disc)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA:** En este caso es muy importante la discretización. Si probáis con 2 bins, veréis como el resultado es mucho peor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 `GaussianNB`\n",
    "\n",
    "`GaussianNB` es la implementación de Naive Bayes para variables continuas estimadas como gausianas. Básicamente, cuenta con los mismos métodos principales que `CategoricalNB`.\n",
    "\n",
    "Vamos a realizar el mismo proceso de prueba con la base `iris` que antes, en este caso sin discretizar las variables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos nuestro `GaussianNB` con `fit(X)`, y calculamos el score del algoritmo con `score(X,y)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentaje de acierto en entrenamiento:  0.9428571428571428\n",
      "Porcentaje de acierto en prueba:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Crear un modelo NB\n",
    "gaussianNB = GaussianNB()\n",
    " \n",
    "# Entrenar el modelo\n",
    "gaussianNB.fit(X_train, y_train)\n",
    "\n",
    "# Calcular el porcentaje de acierto\n",
    "scoreTrain = gaussianNB.score(X_train, y_train)\n",
    "scoreTest = gaussianNB.score(X_test, y_test)\n",
    "print(\"Porcentaje de acierto en entrenamiento: \", scoreTrain)\n",
    "print(\"Porcentaje de acierto en prueba: \", scoreTest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar `predict(X)` para ver las predicciones realizadas para cada instancia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions Train:\n",
      " [1,2,2,1,2,1,2,1,1,2,1,1,2,1,2,1,0,2,1,1,1,1,2,0,0,2,1,0,0,2,0,2,1,0,1,2,1,0,2,2,2,2,0,0,2,2,0,2,0,2,2,0,0,\n",
      " 2,0,0,0,1,2,2,0,0,0,1,1,0,0,1,0,2,1,2,1,0,2,0,2,0,0,2,0,2,1,1,1,2,2,1,2,0,1,2,2,0,1,1,2,1,0,0,0,2,1,2,0] \n",
      "\n",
      "Predictions Test:\n",
      " [2,1,0,2,0,2,0,1,1,1,2,1,1,1,1,0,1,1,0,0,2,1,0,0,2,0,0,1,1,0,2,1,0,2,2,1,0,1,1,1,2,0,2,0,0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predecir las etiquetas de los datos de entrenamiento. \n",
    "predictionsTrain = gaussianNB.predict(X_train)\n",
    "print(\"Predictions Train:\\n\", np.array2string(predictionsTrain, separator=',', max_line_width=107), \"\\n\")\n",
    "\n",
    "# Predecir las etiquetas de los datos de prueba. Mostrar 25 números por linea\n",
    "predictionsTest = gaussianNB.predict(X_test)\n",
    "print(\"Predictions Test:\\n\", np.array2string(predictionsTest, separator=',', max_line_width=107), \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y también podemos ver las probalidades predichas para cada instancia con `predict_proba(X)` (mostramos solo las 5 primeras intancias):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.02882530e-059, 9.99999927e-001, 7.25939137e-008],\n",
       "       [1.07057258e-241, 1.44292097e-003, 9.98557079e-001],\n",
       "       [0.00000000e+000, 3.62956541e-010, 1.00000000e+000],\n",
       "       [4.82432659e-178, 9.51750980e-001, 4.82490196e-002],\n",
       "       [2.04990326e-270, 3.70855956e-004, 9.99629144e-001]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussianNB.predict_proba(X_train)[0:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, podemos ver cómo el rendimiento de `GaussianNB` es mejor que el de `CategoricalNB`. Esto dependerá en gran medida de la base de datos y del discretizado que se realice. Además, la base `iris` es muy pequeña, por lo que estos scores tampoco son representativos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"implementacion\"></a>\n",
    "# 4. `TODO:` Implementación de Naive Bayes mixto\n",
    "\n",
    "<br>\n",
    "\n",
    "En esta sección, implementaremos un clasificador **Naive Bayes mixto**, capaz de manejar tanto variables **categóricas** como **numéricas**. A diferencia de los modelos de `scikit-learn`, que separan `CategoricalNB` y `GaussianNB`, nuestra implementación combinará ambos enfoques en una única clase `NB`, que hereda de `BaseEstimator`. Esto nos permitirá trabajar con datasets que contengan ambos tipos de variables de manera flexible.\n",
    "\n",
    "### Características:\n",
    "\n",
    "1. **Combinación de variables categóricas y numéricas**:\n",
    "   - Las variables categóricas se modelan utilizando probabilidades condicionales discretas.\n",
    "   - Las variables numéricas se modelan utilizando distribuciones gaussianas (media y varianza).\n",
    "\n",
    "2. **Parámetro `categorical_features`**:\n",
    "   - Este parámetro permite especificar qué variables se tratarán como categóricas.\n",
    "   - Puede tomar los siguientes valores:\n",
    "     - Una lista de índices (por ejemplo, `[0, 2, 5]`).\n",
    "     - `'all'` (valor por defecto): Todas las variables se tratan como categóricas.\n",
    "     - `'none'`: Todas las variables se tratan como numéricas.\n",
    "\n",
    "3. **Suavizado de Laplace**:\n",
    "   - Para evitar probabilidades condicionales iguales a cero en las variables categóricas, se aplica el **suavizado de Laplace**.\n",
    "   - Esto es especialmente importante cuando hay categorías en los datos de entrenamiento que no aparecen en algunas clases.\n",
    "   - Si no lo aplicamos, no obtendríamos los mismos resultados que el `CategoricalNB` de `scikit-learn`.\n",
    "\n",
    "4. **Funciones que se deben implementar**: (cada una tiene sus respectivos `TODO`)\n",
    "   - **`fit(self, X, y)`**: Entrena el modelo con los datos de entrenamiento `X` y las etiquetas `y`.\n",
    "   - **`predict(self, X)`**: Predice las etiquetas para los datos de prueba `X`.\n",
    "   - **`predict_proba(self, X)`**: Devuelve las probabilidades de cada clase para los datos de prueba `X`.\n",
    "   - **`score(self, X, y)`**: Calcula la precisión del modelo en los datos de prueba `X` y las etiquetas `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sympy.abc import y\n",
    "\n",
    "# Implementación propia de Naive Bayes que hereda de BaseEstimator siguiendo la estructura de scikit-learn\n",
    "# Las guias de scikit-learn para la implementación de algoritmos se encuentran en https://scikit-learn.org/stable/developers/develop.html\n",
    "class NB(BaseEstimator):\n",
    "    # Constructor\n",
    "    def __init__(self, categorical_features='all'):\n",
    "        self.categorical_features = categorical_features\n",
    "        return\n",
    "\n",
    "    # To string\n",
    "    def __str__(self):\n",
    "        result = \"NB(): Naive Bayes with Categorical and Gaussian Features\\n\\n\"\n",
    "        result += \"P(Y):\\n\" + str(np.round(self.class_prob_, 4)) + \"\\n\\n\"\n",
    "        \n",
    "        for i in range(len(self.tables_)):\n",
    "            if self.is_categorical_[i]:  # Característica categórica\n",
    "                result += f\"P(X{i}|Y):\\n\"\n",
    "                for cls, probs in self.tables_[i].items():\n",
    "                    result += f\"{cls}:\\t{np.round(probs, 4)}\\n\"\n",
    "                result += \"\\n\"\n",
    "            else:  # Característica gaussiana\n",
    "                result += f\"P(X{i}|Y):\\n\"\n",
    "                for cls, (mean, var) in self.tables_[i].items():\n",
    "                    result += f\"{cls}:\\tμ = {mean:.4f}, σ² = {var:.4f}\\n\"\n",
    "                result += \"\\n\"\n",
    "        return result\n",
    "\n",
    "\n",
    "    # Función fit para entrenar el modelo a partir de un conjunto de datos X y sus etiquetas y\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Determinar características categóricas\n",
    "        if self.categorical_features == 'all':\n",
    "            self.categorical_features_ = list(range(n_features))\n",
    "        elif self.categorical_features == 'none':\n",
    "            self.categorical_features_ = []\n",
    "        else:\n",
    "            self.categorical_features_ = self.categorical_features\n",
    "        self.is_categorical_ = np.isin(range(n_features), self.categorical_features_)\n",
    "\n",
    "        # Información de las clases\n",
    "        self.classes_, class_counts = np.unique(y, return_counts=True)\n",
    "        self.n_classes_ = len(self.classes_) \n",
    "        self.class_prob_ = class_counts / n_samples  # P(Y)\n",
    "\n",
    "        # Información de las categorías para las características categóricas\n",
    "        self.categories_ = []\n",
    "        for i in range(n_features):\n",
    "            if self.is_categorical_[i]:\n",
    "                self.categories_.append(np.unique(X[:, i]))\n",
    "            else:\n",
    "                self.categories_.append(None)\n",
    "\n",
    "        self.tables_ = []\n",
    "        for i in range(n_features):\n",
    "            # Probabilidades categóricas con suavizado Laplace\n",
    "            if self.is_categorical_[i]:\n",
    "                # El indice que le asignemos a cada una (A,B,C..) tiene que ser siempre el mismo\n",
    "                # Podria hacerse mediante un diccionario. Nos sirve tb para luego ver que tipo de variable es si discreta (dicc) o numerica (lista)\n",
    "                dic_tablet = {}\n",
    "                \n",
    "                for j in range(len(self.classes_)): # Cada valor posible de y: EJEMPLO: <= y >\n",
    "                    categories = []\n",
    "                    for category in self.categories_[i]: # Cada categoria de la caracteristica i\n",
    "                        # Filtramos por cada clase\n",
    "                        X_cls = X[y == self.classes_[j], i] # X e una matriz de 2 dimensiones, donde cada fila es una instancia de X y cada columna es una característica\n",
    "                        # Calculamos las probabilidades de cada categoría para la clase cls\n",
    "                        \n",
    "                        count = np.sum(X_cls == category)\n",
    "                        categories.append((count + 1) / (len(X_cls) + len(self.categories_[i])))\n",
    "\n",
    "                    dic_tablet[self.classes_[j]] = categories\n",
    "                self.tables_.append(dic_tablet) \n",
    "\n",
    "            # Parámetros gaussianos (media, varianza) CONTINUAS\n",
    "            else:\n",
    "                # TODO: Calcular los parámetros gaussianos (media, varianza)\n",
    "                # Filtramos por cada clase\n",
    "                tablet = [[]*2]*len(self.classes_) # [[varianza y media] n de clases]\n",
    "                for j in range(len(self.classes_)): # Cada valor posible de y: EJEMPLO: <= y >\n",
    "                    #Filtramos por cada clase\n",
    "                    X_cls = X[y == self.classes[j], i] # X e una matriz de 2 dimensiones, donde cada fila es una instancia de X y cada columna es una característica\n",
    "                    # Calculamos la media y la varianza de la característica i para la clase cls\n",
    "                    mean = np.mean(X_cls)\n",
    "                    var = np.var(X_cls)\n",
    "                    tablet[0][j] = mean\n",
    "                    tablet[1][j] = var\n",
    "                self.tables_.append(tablet)\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    # Función predict_proba para predecir las probabilidades de las etiquetas de un conjunto de datos X\n",
    "    def predict_proba(self, X):\n",
    "        num_instances = X.shape[0] # Numero de FILAS de X\n",
    "        \n",
    "        # Inicializar la matriz de probabilidades (una fila por cada instancia de X y una columna por cada posible valor de Y)\n",
    "        prob = np.zeros((num_instances, len(self.classes_)))\n",
    "        p = np.array([])\n",
    "        c = np.array([])\n",
    "        for i in range(num_instances): # Iteramos FILAS\n",
    "            x = X[i, :]\n",
    "            \n",
    "            for k,clss in enumerate(self.classes_): \n",
    "                log_prob_clss = np.log(self.class_prob_[k])\n",
    "                for j in range(X.shape[1]): # Iteramos COLUMNAS dentro de una fila\n",
    "                    if self.is_categorical_[j]: \n",
    "                        for l,category in enumerate(self.categories_[j]):\n",
    "                            if x[j] == category:\n",
    "                                log_prob_clss += np.log(self.tables_[j][clss][l])  \n",
    "                                  \n",
    "                    else:\n",
    "                        # TODO: Calcular la probabilidad de la características gaussianas\n",
    "                        pass\n",
    "\n",
    "                prob[i][k] = log_prob_clss\n",
    "\n",
    "            prob_instancia = np.exp(prob[i] - np.max(prob[i]))\n",
    "            prob[i] = prob_instancia / np.sum(prob_instancia)\n",
    "\n",
    "            c = self.classes_[np.argmax(prob, axis=1)]\n",
    "            p = np.max(prob, axis=1)\n",
    "\n",
    "        return (p,c)\n",
    "    \n",
    "\n",
    "    # TODO: Función predict para predecir las etiquetas de un conjunto de datos X\n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X)[1]\n",
    "\n",
    "\n",
    "    # TODO: Función score para calcular el porcentaje de acierto del modelo a partir de un conjunto de datos X y sus etiquetas y\n",
    "    def score(self, X, y):\n",
    "        correctos = 0\n",
    "        for i in range(len(X)):\n",
    "            if self.predict(X)[i] == y[i]:\n",
    "                correctos += 1\n",
    "        return correctos/len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.seterr(divide='ignore') # Ignorar los avisos de divisiones por 0 que muestra CategoricalNB con alpha=0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"evaluacion\"></a>\n",
    "# 5. Evaluación experimental\n",
    "\n",
    "<br>\n",
    "\n",
    "##  5.1 Pruebas en la BBDD `iris`\n",
    "\n",
    "En primer lugar, vamos a comprobar que efectivamente nuestro nuevo clasificador `NB` funciona correctamente en la base de datos `iris`. Esta base es muy útil para comparar las tablas de probabilidad, ya que tendrán un tamaño pequeño."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  5.1.1 Variables categóricas\n",
    "\n",
    "Primero vamos a probar con las variables categóricas (discretizando):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit es correcto\n",
      "\n",
      "Longitud: 105\n",
      "Longitud esperada: 105\n",
      "\n",
      "Resultado: [1 2 2 1 2 1 2 1 1 1 1 1 2 1 2 1 0 2 1 1 1 1 2 0 0 2 1 0 0 1 0 2 1 0 1 2 1\n",
      " 0 2 2 2 2 0 0 2 2 0 2 0 2 2 0 0 2 0 0 0 1 2 2 0 0 0 1 1 0 0 1 0 2 1 2 1 0\n",
      " 2 0 2 0 0 2 0 2 1 1 1 2 2 1 2 0 1 2 2 0 1 1 2 1 0 0 0 2 1 2 0]\n",
      "Esperado:  [1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 0, 2, 1, 1, 1, 1, 2, 0, 0, 2, 1, 0, 0, 1, 0, 2, 1, 0, 1, 2, 1, 0, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 0, 2, 2, 0, 0, 2, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 1, 0, 0, 1, 0, 2, 1, 2, 1, 0, 2, 0, 2, 0, 0, 2, 0, 2, 1, 1, 1, 2, 2, 1, 2, 0, 1, 2, 2, 0, 1, 1, 2, 1, 0, 0, 0, 2, 1, 2, 0]\n",
      "Los predict son correctos: True\n",
      "\n",
      "Score train: 0.9428571428571428\n",
      "Score test:  0.9333333333333333 \n",
      "\n",
      "\n",
      "Predictions train:\n",
      " [1,2,2,1,2,1,2,1,1,1,1,1,2,1,2,1,0,2,1,1,1,1,2,0,0,2,1,0,0,1,0,2,1,0,1,2,1,0,2,2,2,2,0,0,2,2,0,2,0,2,2,0,0,\n",
      " 2,0,0,0,1,2,2,0,0,0,1,1,0,0,1,0,2,1,2,1,0,2,0,2,0,0,2,0,2,1,1,1,2,2,1,2,0,1,2,2,0,1,1,2,1,0,0,0,2,1,2,0] \n",
      "\n",
      "Predictions test:\n",
      " [2,1,0,2,0,2,0,1,1,1,1,1,1,1,1,0,1,1,0,0,1,1,0,0,1,0,0,1,1,0,2,1,0,2,2,1,0,1,1,1,2,0,2,0,0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "expected = [1,2,2,1,2,1,2,1,1,1,1,1,2,1,2,1,0,2,1,1,1,1,2,0,0,2,1,0,0,1,0,2,1,0,1,2,1,0,2,2,2,2,0,0,2,2,0,2,0,2,2,0,0,2,0,0,0,1,2,2,0,0,0,1,1,0,0,1,0,2,1,2,1,0,2,0,2,0,0,2,0,2,1,1,1,2,2,1,2,0,1,2,2,0,1,1,2,1,0,0,0,2,1,2,0]\n",
    "\n",
    "# Crear un modelo NB categórico\n",
    "nb = NB(categorical_features='all')\n",
    "\n",
    "# Entrenar el modelo\n",
    "nb.fit(X_train_disc, y_train)\n",
    "#utils.see_probability_tables(nb)\n",
    "print(\"Fit es correcto\")\n",
    "p = nb.predict(X_train_disc)\n",
    "\n",
    "print(\"\\nLongitud:\",len(p))\n",
    "print(\"Longitud esperada:\",len(expected))\n",
    "\n",
    "print(\"\\nResultado:\",p)\n",
    "print(\"Esperado: \",expected)\n",
    "print(\"Los predict son correctos:\",np.any(expected == p))\n",
    "\n",
    "#print(y_train)\n",
    "# Calcular el porcentaje de acierto\n",
    "print(\"\\nScore train:\", nb.score(X_train_disc, y_train))\n",
    "print(\"Score test: \", nb.score(X_test_disc, y_test), \"\\n\")\n",
    "\n",
    "# Predecir las etiquetas de los datos de entrenamiento. \n",
    "print(\"\\nPredictions train:\\n\", np.array2string(nb.predict(X_train_disc), separator=',', max_line_width=107), \"\\n\")\n",
    "print(\"Predictions test:\\n\", np.array2string(nb.predict(X_test_disc), separator=',', max_line_width=107), \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el método `see_probability_tables(nb)` de `utils` podemos ver las tablas de probabilidades que ha creado el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Y):\n",
      " [0.32380952 0.3047619  0.37142857] \n",
      "\n",
      "P(X0|Y):\n",
      "0:\t[0.8649 0.1081 0.027 ]\n",
      "1:\t[0.1143 0.7429 0.1429]\n",
      "2:\t[0.0476 0.4762 0.4762]\n",
      "\n",
      "P(X1|Y):\n",
      "0:\t[0.0541 0.7027 0.2432]\n",
      "1:\t[0.5143 0.4571 0.0286]\n",
      "2:\t[0.3571 0.5714 0.0714]\n",
      "\n",
      "P(X2|Y):\n",
      "0:\t[0.9459 0.027  0.027 ]\n",
      "1:\t[0.0571 0.8857 0.0571]\n",
      "2:\t[0.0238 0.0952 0.881 ]\n",
      "\n",
      "P(X3|Y):\n",
      "0:\t[0.9459 0.027  0.027 ]\n",
      "1:\t[0.0286 0.8857 0.0857]\n",
      "2:\t[0.0238 0.0952 0.881 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utils.see_probability_tables(nb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora podemos realizar lo mismo con `CategoricalNB` para comprobar que los resultados son los mismos: \n",
    "\n",
    "NOTA: Si no implementamos el suavizado de Laplace, deberemos llamar a `CategoricalNB(alpha=0, force_alpha=True)` para obtener el mismo resultado. Esto puede que no os funcione dependiendo de vuestra versión de `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train: 0.9428571428571428\n",
      "Score test:  0.9333333333333333 \n",
      "\n",
      "Predictions train:\n",
      " [1,2,2,1,2,1,2,1,1,1,1,1,2,1,2,1,0,2,1,1,1,1,2,0,0,2,1,0,0,1,0,2,1,0,1,2,1,0,2,2,2,2,0,0,2,2,0,2,0,2,2,0,0,\n",
      " 2,0,0,0,1,2,2,0,0,0,1,1,0,0,1,0,2,1,2,1,0,2,0,2,0,0,2,0,2,1,1,1,2,2,1,2,0,1,2,2,0,1,1,2,1,0,0,0,2,1,2,0] \n",
      "\n",
      "Predictions test:\n",
      " [2,1,0,2,0,2,0,1,1,1,1,1,1,1,1,0,1,1,0,0,1,1,0,0,1,0,0,1,1,0,2,1,0,2,2,1,0,1,1,1,2,0,2,0,0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comparar con el modelo CategoricalNB de scikit\n",
    "nb = CategoricalNB()\n",
    "nb.fit(X_train_disc, y_train)\n",
    "\n",
    "# Calcular el porcentaje de acierto\n",
    "print(\"Score train:\", nb.score(X_train_disc, y_train))\n",
    "print(\"Score test: \", nb.score(X_test_disc, y_test), \"\\n\")\n",
    "\n",
    "# Predecir las etiquetas de los datos de entrenamiento. \n",
    "print(\"Predictions train:\\n\", np.array2string(nb.predict(X_train_disc), separator=',', max_line_width=107), \"\\n\")\n",
    "print(\"Predictions test:\\n\", np.array2string(nb.predict(X_test_disc), separator=',', max_line_width=107), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Y):\n",
      " [0.32380952 0.3047619  0.37142857] \n",
      "\n",
      "P(X0|Y):\n",
      "0:\t[0.8649 0.1081 0.027 ]\n",
      "1:\t[0.1143 0.7429 0.1429]\n",
      "2:\t[0.0476 0.4762 0.4762]\n",
      "\n",
      "P(X1|Y):\n",
      "0:\t[0.0541 0.7027 0.2432]\n",
      "1:\t[0.5143 0.4571 0.0286]\n",
      "2:\t[0.3571 0.5714 0.0714]\n",
      "\n",
      "P(X2|Y):\n",
      "0:\t[0.9459 0.027  0.027 ]\n",
      "1:\t[0.0571 0.8857 0.0571]\n",
      "2:\t[0.0238 0.0952 0.881 ]\n",
      "\n",
      "P(X3|Y):\n",
      "0:\t[0.9459 0.027  0.027 ]\n",
      "1:\t[0.0286 0.8857 0.0857]\n",
      "2:\t[0.0238 0.0952 0.881 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utils.see_probability_tables(nb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos comprobar, el resultado es el mismo. Si queremos comprobar más bins, podemos utilizar la función `compare_two_models` de utils para comparar los accuracy en test de `NB` y `CategoricalNB` con distintos discretizados, en este caso igual anchura desde 2 hasta 10 bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Variables continuas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train: 0.9428571428571428\n",
      "Score test:  1.0 \n",
      "\n",
      "Predictions train:\n",
      " [1,2,2,1,2,1,2,1,1,2,1,1,2,1,2,1,0,2,1,1,1,1,2,0,0,2,1,0,0,2,0,2,1,0,1,2,1,0,2,2,2,2,0,0,2,2,0,2,0,2,2,0,0,\n",
      " 2,0,0,0,1,2,2,0,0,0,1,1,0,0,1,0,2,1,2,1,0,2,0,2,0,0,2,0,2,1,1,1,2,2,1,2,0,1,2,2,0,1,1,2,1,0,0,0,2,1,2,0] \n",
      "\n",
      "Predictions test:\n",
      " [2,1,0,2,0,2,0,1,1,1,2,1,1,1,1,0,1,1,0,0,2,1,0,0,2,0,0,1,1,0,2,1,0,2,2,1,0,1,1,1,2,0,2,0,0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear un modelo NB categórico\n",
    "nb = NB(categorical_features='none')\n",
    "\n",
    "# Entrenar el modelo\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Calcular el porcentaje de acierto\n",
    "print(\"Score train:\", nb.score(X_train, y_train))\n",
    "print(\"Score test: \", nb.score(X_test, y_test), \"\\n\")\n",
    "\n",
    "# Predecir las etiquetas de los datos de entrenamiento. \n",
    "print(\"Predictions train:\\n\", np.array2string(nb.predict(X_train), separator=',', max_line_width=107), \"\\n\")\n",
    "print(\"Predictions test:\\n\", np.array2string(nb.predict(X_test), separator=',', max_line_width=107), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Y):\n",
      " [0.32380952 0.3047619  0.37142857] \n",
      "\n",
      "P(X0|Y):\n",
      "0:\tμ = 4.9941, σ² = 0.1297\n",
      "1:\tμ = 5.9219, σ² = 0.2902\n",
      "2:\tμ = 6.6538, σ² = 0.4373\n",
      "\n",
      "P(X1|Y):\n",
      "0:\tμ = 3.3824, σ² = 0.1579\n",
      "1:\tμ = 2.7563, σ² = 0.1193\n",
      "2:\tμ = 2.9872, σ² = 0.1233\n",
      "\n",
      "P(X2|Y):\n",
      "0:\tμ = 1.4529, σ² = 0.0207\n",
      "1:\tμ = 4.1969, σ² = 0.2416\n",
      "2:\tμ = 5.5974, σ² = 0.3224\n",
      "\n",
      "P(X3|Y):\n",
      "0:\tμ = 0.2324, σ² = 0.0089\n",
      "1:\tμ = 1.3062, σ² = 0.0432\n",
      "2:\tμ = 2.0308, σ² = 0.0690\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utils.see_probability_tables(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train: 0.9428571428571428\n",
      "Score test:  1.0 \n",
      "\n",
      "Predictions train:\n",
      " [1,2,2,1,2,1,2,1,1,2,1,1,2,1,2,1,0,2,1,1,1,1,2,0,0,2,1,0,0,2,0,2,1,0,1,2,1,0,2,2,2,2,0,0,2,2,0,2,0,2,2,0,0,\n",
      " 2,0,0,0,1,2,2,0,0,0,1,1,0,0,1,0,2,1,2,1,0,2,0,2,0,0,2,0,2,1,1,1,2,2,1,2,0,1,2,2,0,1,1,2,1,0,0,0,2,1,2,0] \n",
      "\n",
      "Predictions test:\n",
      " [2,1,0,2,0,2,0,1,1,1,2,1,1,1,1,0,1,1,0,0,2,1,0,0,2,0,0,1,1,0,2,1,0,2,2,1,0,1,1,1,2,0,2,0,0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comparar con el modelo GaussianNB de scikit\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Calcular el porcentaje de acierto\n",
    "print(\"Score train:\", nb.score(X_train, y_train))\n",
    "print(\"Score test: \", nb.score(X_test, y_test), \"\\n\")\n",
    "\n",
    "# Predecir las etiquetas de los datos de entrenamiento. \n",
    "print(\"Predictions train:\\n\", np.array2string(nb.predict(X_train), separator=',', max_line_width=107), \"\\n\")\n",
    "print(\"Predictions test:\\n\", np.array2string(nb.predict(X_test), separator=',', max_line_width=107), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Y):\n",
      " [0.32380952 0.3047619  0.37142857] \n",
      "\n",
      "P(X0|Y):\n",
      "0:\tμ = 4.9941, σ² = 0.1258\n",
      "1:\tμ = 5.9219, σ² = 0.2811\n",
      "2:\tμ = 6.6538, σ² = 0.4261\n",
      "\n",
      "P(X1|Y):\n",
      "0:\tμ = 3.3824, σ² = 0.1532\n",
      "1:\tμ = 2.7563, σ² = 0.1156\n",
      "2:\tμ = 2.9872, σ² = 0.1201\n",
      "\n",
      "P(X2|Y):\n",
      "0:\tμ = 1.4529, σ² = 0.0201\n",
      "1:\tμ = 4.1969, σ² = 0.2341\n",
      "2:\tμ = 5.5974, σ² = 0.3141\n",
      "\n",
      "P(X3|Y):\n",
      "0:\tμ = 0.2324, σ² = 0.0087\n",
      "1:\tμ = 1.3062, σ² = 0.0418\n",
      "2:\tμ = 2.0308, σ² = 0.0673\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utils.see_probability_tables(nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Pruebas con la BBDD `adult`\n",
    "\n",
    "Ahora, vamos a utilizar la base de datos `adult` que hemos utilizado en la Práctica 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education-num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital-status         occupation   relationship   race     sex  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country income  \n",
       "0          2174             0              40  United-States  <=50K  \n",
       "1             0             0              13  United-States  <=50K  \n",
       "2             0             0              40  United-States  <=50K  \n",
       "3             0             0              40  United-States  <=50K  \n",
       "4             0             0              40           Cuba  <=50K  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_adult = pd.read_csv('adult.csv')\n",
    "df_adult_test = pd.read_csv('adult_test.csv')\n",
    "df_adult.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para simplificar, vamos a obtener en una sola celda varias combinaciones con las cuales probar el Naive Bayes:\n",
    "- `X_train`, `X_test` con todas las variables\n",
    "- `X_train_cont`, `X_test_cont` con solo las variables continuas\n",
    "- `X_train_cat`, `X_test_cat` con solo las variables categóricas\n",
    "- `X_train_onehot`, `X_test_onehot` con las variables numéricas, y las categóricas codificadas con one-hot (para poder entrenar por ejemplo el Naive Bayes gaussiano)\n",
    "- `X_train_disc`, `X_test_disc` con las variables numéricas, y las categóricas discretizadas (para poder entrenar por ejemplo el Naive Bayes categórico)\n",
    "- `y_train`, `y_test` con la variable objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables continuas, categóricas y objetivo\n",
    "cont_vars = [\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
    "cont_vars_idx = [df_adult.columns.get_loc(var) for var in cont_vars]\n",
    "\n",
    "cat_vars = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"]\n",
    "cat_vars_idx = [df_adult.columns.get_loc(var) for var in cat_vars]\n",
    "\n",
    "target_var = \"income\"\n",
    "\n",
    "# Número de bins para la discretización\n",
    "nBins = 5\n",
    "\n",
    "# X_train, X_test con todas las variables\n",
    "X_train = df_adult.drop(columns=[target_var]).values\n",
    "X_test = df_adult_test.drop(columns=[target_var]).values\n",
    "\n",
    "# Ordinal enconding de las categóricas para que no haya problemas con los strings. Ej: \"workclass\" -> \"State-gov\" -> 0, \"Self-emp-not-inc\" -> 1, ...\n",
    "enc = OrdinalEncoder()\n",
    "X_train[:, cat_vars_idx] = enc.fit_transform(X_train[:, cat_vars_idx])\n",
    "X_test[:, cat_vars_idx] = enc.transform(X_test[:, cat_vars_idx])\n",
    "\n",
    "# X_train_cont, X_test_cont con solo las variables continuas\n",
    "X_train_cont = X_train[:, cont_vars_idx]\n",
    "X_test_cont = X_test[:, cont_vars_idx]\n",
    "\n",
    "# X_train_cat, X_test_cat con solo las variables categóricas\n",
    "X_train_cat = X_train[:, cat_vars_idx]\n",
    "X_test_cat = X_test[:, cat_vars_idx]\n",
    "\n",
    "# X_train_onehot, X_test_onehot con las variables numéricas, y las categóricas codificadas con one-hot (simulamos todas numéricas)\n",
    "df_train_onehot = df_adult.drop(columns=[target_var])\n",
    "df_test_onehot = df_adult_test.drop(columns=[target_var])\n",
    "\n",
    "X_train_onehot = pd.get_dummies(df_train_onehot, columns=cat_vars).values\n",
    "X_test_onehot = pd.get_dummies(df_test_onehot, columns=cat_vars).values\n",
    "\n",
    "# X_train_disc, X_test_disc con las variables numéricas, y las categóricas discretizadas en nBins (simulamos todas categóricas)\n",
    "discretizer = KBinsDiscretizer(n_bins=nBins, encode='ordinal', strategy='uniform')\n",
    "X_train_disc = discretizer.fit_transform(X_train_cont)\n",
    "X_test_disc = discretizer.transform(X_test_cont)\n",
    "\n",
    "# y_train, y_test con la variable objetivo\n",
    "y_train = df_adult[target_var]\n",
    "y_test = df_adult_test[target_var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Prueba del NB mixto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train: 0.8337274653726852 \n",
      "Score test:  0.8311528775873718\n"
     ]
    }
   ],
   "source": [
    "# Crear un modelo NB mixto\n",
    "nb = NB(categorical_features=cat_vars_idx)\n",
    "nb.fit(X_train, y_train)\n",
    "print(\"Score train:\", nb.score(X_train, y_train),\"\\nScore test: \", nb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Y):\n",
      " [0.75919044 0.24080956] \n",
      "\n",
      "P(X0|Y):\n",
      "<=50K:\tμ = 36.7837, σ² = 196.5629\n",
      ">50K:\tμ = 44.2498, σ² = 110.6499\n",
      "\n",
      "P(X1|Y):\n",
      "<=50K:\t[6.660e-02 2.390e-02 5.970e-02 3.000e-04 7.171e-01 2.000e-02 7.350e-02\n",
      " 3.830e-02 6.000e-04]\n",
      ">50K:\t[2.450e-02 4.740e-02 7.870e-02 1.000e-04 6.324e-01 7.940e-02 9.240e-02\n",
      " 4.510e-02 1.000e-04]\n",
      "\n",
      "P(X2|Y):\n",
      "<=50K:\tμ = 190340.8652, σ² = 11338474078.7776\n",
      ">50K:\tμ = 188005.0000, σ² = 10514815717.0130\n",
      "\n",
      "P(X3|Y):\n",
      "<=50K:\t[0.0353 0.0451 0.0162 0.0066 0.0129 0.0245 0.0197 0.0325 0.0413 0.1267\n",
      " 0.0044 0.3568 0.0309 0.0021 0.0062 0.2387]\n",
      ">50K:\t[8.000e-03 7.800e-03 4.300e-03 9.000e-04 2.200e-03 5.200e-03 3.600e-03\n",
      " 3.390e-02 4.610e-02 2.828e-01 3.910e-02 2.133e-01 1.222e-01 1.000e-04\n",
      " 5.400e-02 1.767e-01]\n",
      "\n",
      "P(X4|Y):\n",
      "<=50K:\tμ = 9.5951, σ² = 5.9348\n",
      ">50K:\tμ = 11.6117, σ² = 5.6888\n",
      "\n",
      "P(X5|Y):\n",
      "<=50K:\t[0.161  0.0006 0.3351 0.0156 0.4122 0.0388 0.0368]\n",
      ">50K:\t[0.0591 0.0014 0.8528 0.0045 0.0627 0.0085 0.011 ]\n",
      "\n",
      "P(X6|Y):\n",
      "<=50K:\t[0.0668 0.132  0.0004 0.1282 0.0849 0.0356 0.052  0.0709 0.1277 0.006\n",
      " 0.0923 0.0177 0.1079 0.0261 0.0517]\n",
      ">50K:\t[0.0244 0.0647 0.0003 0.1184 0.2506 0.0148 0.0111 0.032  0.0176 0.0003\n",
      " 0.2368 0.027  0.1253 0.0362 0.0409]\n",
      "\n",
      "P(X7|Y):\n",
      "<=50K:\t[0.2943 0.3013 0.0382 0.2023 0.1306 0.0333]\n",
      ">50K:\t[0.7543 0.1092 0.0048 0.0087 0.0279 0.0951]\n",
      "\n",
      "P(X8|Y):\n",
      "<=50K:\t[0.0112 0.0309 0.1107 0.01   0.8372]\n",
      ">50K:\t[0.0047 0.0353 0.0495 0.0033 0.9072]\n",
      "\n",
      "P(X9|Y):\n",
      "<=50K:\t[0.388 0.612]\n",
      ">50K:\t[0.1505 0.8495]\n",
      "\n",
      "P(X10|Y):\n",
      "<=50K:\tμ = 148.7525, σ² = 927637.3254\n",
      ">50K:\tμ = 4006.1425, σ² = 212295942.7839\n",
      "\n",
      "P(X11|Y):\n",
      "<=50K:\tμ = 53.1429, σ² = 96569.1480\n",
      ">50K:\tμ = 195.0015, σ² = 354605.4508\n",
      "\n",
      "P(X12|Y):\n",
      "<=50K:\tμ = 38.8402, σ² = 151.7576\n",
      ">50K:\tμ = 45.4730, σ² = 121.2855\n",
      "\n",
      "P(X13|Y):\n",
      "<=50K:\t[1.770e-02 5.000e-04 3.400e-03 2.300e-03 2.300e-03 2.900e-03 2.800e-03\n",
      " 1.000e-03 4.000e-03 2.500e-03 7.000e-04 3.800e-03 9.000e-04 2.500e-03\n",
      " 1.700e-03 1.000e-04 5.000e-04 6.000e-04 4.000e-04 2.500e-03 1.000e-03\n",
      " 8.000e-04 2.000e-03 2.900e-03 1.600e-03 7.000e-04 2.470e-02 1.300e-03\n",
      " 6.000e-04 1.200e-03 5.600e-03 2.000e-03 1.400e-03 4.200e-03 4.000e-04\n",
      " 2.600e-03 1.300e-03 6.000e-04 7.000e-04 8.885e-01 2.500e-03 4.000e-04]\n",
      ">50K:\t[1.860e-02 1.000e-03 5.100e-03 2.700e-03 4.000e-04 3.300e-03 4.000e-04\n",
      " 6.000e-04 1.300e-03 3.900e-03 1.600e-03 5.700e-03 1.100e-03 5.000e-04\n",
      " 6.000e-04 1.000e-04 3.000e-04 9.000e-04 5.000e-04 5.200e-03 2.400e-03\n",
      " 8.000e-04 3.300e-03 1.400e-03 3.200e-03 4.000e-04 4.300e-03 4.000e-04\n",
      " 1.000e-04 4.000e-04 7.900e-03 1.600e-03 6.000e-04 1.600e-03 5.000e-04\n",
      " 2.200e-03 2.700e-03 5.000e-04 4.000e-04 9.098e-01 8.000e-04 9.000e-04]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utils.see_probability_tables(nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 `TODO:` Experimentación exhaustiva\n",
    "\n",
    "En esta sección, se propone realizar una experimentación exhaustiva con el modelo `NB` implementado, comparándolo con los modelos de `scikit-learn` y con los árboles de decisión estudiados en la Práctica 1. El objetivo es entender el comportamiento del Naive Bayes mixto en diferentes escenarios y sacar conclusiones sobre su rendimiento. \n",
    "\n",
    "Para ello, se puede probar el modelo `NB` en diferentes configuraciones: solo variables categóricas (`X_train_cat`), solo variables numéricas (`X_train_cont`), una combinación de ambas (`X_train`), discretizando las numéricas (`X_train_disc`) o aplicando un one-hot a las categóricas (`X_train_onehot`).\n",
    "\n",
    "Obtén métricas como la tasa de acierto (en train y test) o el tiempo de ejecución. Además, también se pueden obtener métricas adicionales como precisión, recall o F1-score, ya que la base de datos está desbalanceada. Compara los resultados con los obtenidos en la Práctica 1 y analiza qué modelo funciona mejor."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Practica 1 - Arboles de decision.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "VirtualEnvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
